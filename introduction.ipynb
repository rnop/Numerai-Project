{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "introduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOdH0cHaRZ6D+Kf0TJm4djr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rnop/nmr_tournament/blob/main/introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz1q_3GYLSSm"
      },
      "source": [
        "### Introduction to the Numerai Tournament\n",
        "Numerai is a hedge fund that trades the global markets based on models created by data scientists all over the world. Numerai is unique in that it provides free high-quality financial datasets that are worth millions of dollars to any user wanting to participate in their tournament. Users are able to build their own models on this anonymized and obfuscated dataset, submit their predictions, and follow their investment performance on the live stock market. If users are confident about their models, they are able to stake on them with real money using Numerai's cryptocurrency, Numeraire (NMR).\n",
        "\n",
        "### About this Notebook\n",
        "The purpose of this notebook is to provide an introduction on how to approach the main Numerai tournament.\n",
        "\n",
        "What's included:\n",
        "* how to read in the Numerai data via API\n",
        "* approaches to dimensionality reduction\n",
        "* training an xgboost model \n",
        "* bayesian optimization techniques\n",
        "* calculating predictions from the current round\n",
        "\n",
        "### Disclaimer\n",
        "**This model is not guaranteed to make you money.** I am currently not staking this particular model in the tournament. This notebook only serves to provide you an introduction to the tournament and to give some of my personal input on how to tackle this data science problem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSJ50dNE3iYR",
        "outputId": "1f886795-82e9-48c5-fbbf-1f9c66d146df"
      },
      "source": [
        "# Download the numerai library \n",
        "! pip install numerapi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numerapi in /usr/local/lib/python3.7/dist-packages (2.4.5)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from numerapi) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from numerapi) (2018.9)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2020.12.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->numerapi) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9oubZY336Lv"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numerapi"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7ziKsCr4NBy"
      },
      "source": [
        "### Import data\n",
        "* Training data contains 501,808 observations\n",
        "* Tournament data contains the testing and validation sets, and the live observations you need to predict on for the upcoming round"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSm76usa4KtR",
        "outputId": "0c508e95-7123-4292-fb64-a0a0cac7d2bf"
      },
      "source": [
        "host = 'numerai-public-datasets.s3-us-west-2.amazonaws.com'\n",
        "train_filename = 'latest_numerai_training_data.csv.xz'\n",
        "tourney_filename = 'latest_numerai_tournament_data.csv.xz'\n",
        "\n",
        "train_df = pd.read_csv('https://{}/{}'.format(host, train_filename))\n",
        "tourney_df = pd.read_csv('https://{}/{}'.format(host, tourney_filename))\n",
        "\n",
        "#Confirm round number\n",
        "napi = numerapi.NumerAPI(verbosity=\"info\")\n",
        "current_round = napi.get_current_round()\n",
        "print()\n",
        "print(\"ROUND NUMBER: \", current_round)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "ROUND NUMBER:  260\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "EcpHlJib4Tg4",
        "outputId": "c1702513-57fa-4d23-9dfe-f9ab7cfa7469"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>era</th>\n",
              "      <th>data_type</th>\n",
              "      <th>feature_intelligence1</th>\n",
              "      <th>feature_intelligence2</th>\n",
              "      <th>feature_intelligence3</th>\n",
              "      <th>feature_intelligence4</th>\n",
              "      <th>feature_intelligence5</th>\n",
              "      <th>feature_intelligence6</th>\n",
              "      <th>feature_intelligence7</th>\n",
              "      <th>feature_intelligence8</th>\n",
              "      <th>feature_intelligence9</th>\n",
              "      <th>feature_intelligence10</th>\n",
              "      <th>feature_intelligence11</th>\n",
              "      <th>feature_intelligence12</th>\n",
              "      <th>feature_charisma1</th>\n",
              "      <th>feature_charisma2</th>\n",
              "      <th>feature_charisma3</th>\n",
              "      <th>feature_charisma4</th>\n",
              "      <th>feature_charisma5</th>\n",
              "      <th>feature_charisma6</th>\n",
              "      <th>feature_charisma7</th>\n",
              "      <th>feature_charisma8</th>\n",
              "      <th>feature_charisma9</th>\n",
              "      <th>feature_charisma10</th>\n",
              "      <th>feature_charisma11</th>\n",
              "      <th>feature_charisma12</th>\n",
              "      <th>feature_charisma13</th>\n",
              "      <th>feature_charisma14</th>\n",
              "      <th>feature_charisma15</th>\n",
              "      <th>feature_charisma16</th>\n",
              "      <th>feature_charisma17</th>\n",
              "      <th>feature_charisma18</th>\n",
              "      <th>feature_charisma19</th>\n",
              "      <th>feature_charisma20</th>\n",
              "      <th>feature_charisma21</th>\n",
              "      <th>feature_charisma22</th>\n",
              "      <th>feature_charisma23</th>\n",
              "      <th>feature_charisma24</th>\n",
              "      <th>feature_charisma25</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_wisdom8</th>\n",
              "      <th>feature_wisdom9</th>\n",
              "      <th>feature_wisdom10</th>\n",
              "      <th>feature_wisdom11</th>\n",
              "      <th>feature_wisdom12</th>\n",
              "      <th>feature_wisdom13</th>\n",
              "      <th>feature_wisdom14</th>\n",
              "      <th>feature_wisdom15</th>\n",
              "      <th>feature_wisdom16</th>\n",
              "      <th>feature_wisdom17</th>\n",
              "      <th>feature_wisdom18</th>\n",
              "      <th>feature_wisdom19</th>\n",
              "      <th>feature_wisdom20</th>\n",
              "      <th>feature_wisdom21</th>\n",
              "      <th>feature_wisdom22</th>\n",
              "      <th>feature_wisdom23</th>\n",
              "      <th>feature_wisdom24</th>\n",
              "      <th>feature_wisdom25</th>\n",
              "      <th>feature_wisdom26</th>\n",
              "      <th>feature_wisdom27</th>\n",
              "      <th>feature_wisdom28</th>\n",
              "      <th>feature_wisdom29</th>\n",
              "      <th>feature_wisdom30</th>\n",
              "      <th>feature_wisdom31</th>\n",
              "      <th>feature_wisdom32</th>\n",
              "      <th>feature_wisdom33</th>\n",
              "      <th>feature_wisdom34</th>\n",
              "      <th>feature_wisdom35</th>\n",
              "      <th>feature_wisdom36</th>\n",
              "      <th>feature_wisdom37</th>\n",
              "      <th>feature_wisdom38</th>\n",
              "      <th>feature_wisdom39</th>\n",
              "      <th>feature_wisdom40</th>\n",
              "      <th>feature_wisdom41</th>\n",
              "      <th>feature_wisdom42</th>\n",
              "      <th>feature_wisdom43</th>\n",
              "      <th>feature_wisdom44</th>\n",
              "      <th>feature_wisdom45</th>\n",
              "      <th>feature_wisdom46</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>n000315175b67977</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>n0014af834a96cdd</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>n001c93979ac41d4</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>n0034e4143f22a13</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>n00679d1a636062f</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 314 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id   era data_type  ...  feature_wisdom45  feature_wisdom46  target\n",
              "0  n000315175b67977  era1     train  ...              0.50              0.75    0.50\n",
              "1  n0014af834a96cdd  era1     train  ...              0.25              1.00    0.25\n",
              "2  n001c93979ac41d4  era1     train  ...              0.25              0.75    0.25\n",
              "3  n0034e4143f22a13  era1     train  ...              1.00              1.00    0.25\n",
              "4  n00679d1a636062f  era1     train  ...              0.25              0.75    0.75\n",
              "\n",
              "[5 rows x 314 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "Skw9H8Sd4xJs",
        "outputId": "467817b9-6648-49b4-e2f7-28b853eadc71"
      },
      "source": [
        "tourney_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>era</th>\n",
              "      <th>data_type</th>\n",
              "      <th>feature_intelligence1</th>\n",
              "      <th>feature_intelligence2</th>\n",
              "      <th>feature_intelligence3</th>\n",
              "      <th>feature_intelligence4</th>\n",
              "      <th>feature_intelligence5</th>\n",
              "      <th>feature_intelligence6</th>\n",
              "      <th>feature_intelligence7</th>\n",
              "      <th>feature_intelligence8</th>\n",
              "      <th>feature_intelligence9</th>\n",
              "      <th>feature_intelligence10</th>\n",
              "      <th>feature_intelligence11</th>\n",
              "      <th>feature_intelligence12</th>\n",
              "      <th>feature_charisma1</th>\n",
              "      <th>feature_charisma2</th>\n",
              "      <th>feature_charisma3</th>\n",
              "      <th>feature_charisma4</th>\n",
              "      <th>feature_charisma5</th>\n",
              "      <th>feature_charisma6</th>\n",
              "      <th>feature_charisma7</th>\n",
              "      <th>feature_charisma8</th>\n",
              "      <th>feature_charisma9</th>\n",
              "      <th>feature_charisma10</th>\n",
              "      <th>feature_charisma11</th>\n",
              "      <th>feature_charisma12</th>\n",
              "      <th>feature_charisma13</th>\n",
              "      <th>feature_charisma14</th>\n",
              "      <th>feature_charisma15</th>\n",
              "      <th>feature_charisma16</th>\n",
              "      <th>feature_charisma17</th>\n",
              "      <th>feature_charisma18</th>\n",
              "      <th>feature_charisma19</th>\n",
              "      <th>feature_charisma20</th>\n",
              "      <th>feature_charisma21</th>\n",
              "      <th>feature_charisma22</th>\n",
              "      <th>feature_charisma23</th>\n",
              "      <th>feature_charisma24</th>\n",
              "      <th>feature_charisma25</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_wisdom8</th>\n",
              "      <th>feature_wisdom9</th>\n",
              "      <th>feature_wisdom10</th>\n",
              "      <th>feature_wisdom11</th>\n",
              "      <th>feature_wisdom12</th>\n",
              "      <th>feature_wisdom13</th>\n",
              "      <th>feature_wisdom14</th>\n",
              "      <th>feature_wisdom15</th>\n",
              "      <th>feature_wisdom16</th>\n",
              "      <th>feature_wisdom17</th>\n",
              "      <th>feature_wisdom18</th>\n",
              "      <th>feature_wisdom19</th>\n",
              "      <th>feature_wisdom20</th>\n",
              "      <th>feature_wisdom21</th>\n",
              "      <th>feature_wisdom22</th>\n",
              "      <th>feature_wisdom23</th>\n",
              "      <th>feature_wisdom24</th>\n",
              "      <th>feature_wisdom25</th>\n",
              "      <th>feature_wisdom26</th>\n",
              "      <th>feature_wisdom27</th>\n",
              "      <th>feature_wisdom28</th>\n",
              "      <th>feature_wisdom29</th>\n",
              "      <th>feature_wisdom30</th>\n",
              "      <th>feature_wisdom31</th>\n",
              "      <th>feature_wisdom32</th>\n",
              "      <th>feature_wisdom33</th>\n",
              "      <th>feature_wisdom34</th>\n",
              "      <th>feature_wisdom35</th>\n",
              "      <th>feature_wisdom36</th>\n",
              "      <th>feature_wisdom37</th>\n",
              "      <th>feature_wisdom38</th>\n",
              "      <th>feature_wisdom39</th>\n",
              "      <th>feature_wisdom40</th>\n",
              "      <th>feature_wisdom41</th>\n",
              "      <th>feature_wisdom42</th>\n",
              "      <th>feature_wisdom43</th>\n",
              "      <th>feature_wisdom44</th>\n",
              "      <th>feature_wisdom45</th>\n",
              "      <th>feature_wisdom46</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>n0003aa52cab36c2</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>n000920ed083903f</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>n0038e640522c4a6</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>n004ac94a87dc54b</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>n0052fe97ea0c05f</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 314 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id     era  ... feature_wisdom46  target\n",
              "0  n0003aa52cab36c2  era121  ...             0.00    0.25\n",
              "1  n000920ed083903f  era121  ...             0.50    0.50\n",
              "2  n0038e640522c4a6  era121  ...             0.00    1.00\n",
              "3  n004ac94a87dc54b  era121  ...             0.25    0.50\n",
              "4  n0052fe97ea0c05f  era121  ...             1.00    0.75\n",
              "\n",
              "[5 rows x 314 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y8F2NgN4400",
        "outputId": "808e6fde-03c3-4fea-bbd3-7de49d0d5815"
      },
      "source": [
        "# Number of observations in each dataset type\n",
        "tourney_df['data_type'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "test          1555267\n",
              "validation     137779\n",
              "live             5435\n",
              "Name: data_type, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu9JcWlX5cnK"
      },
      "source": [
        "### Data Preprocessing\n",
        "Most of the data cleaning has been done by Numerai in order to anonymize and obfuscate the data to us. This is done purposefully because of the data sharing rights from the data vendors Numerai spends millions of dollars on (thank you Numerai!). \n",
        "\n",
        "Steps:\n",
        "* Extract features\n",
        "* Convert era from string to integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-0pxeCF5Cjd",
        "outputId": "8d9a8a15-321b-46be-8fee-b03f9d53731a"
      },
      "source": [
        "# Extract features\n",
        "tourney_ids = tourney_df['id']\n",
        "features = [c for c in tourney_df if c.startswith('feature')]\n",
        "\n",
        "# The training data is also grouped into 120 different eras (1-120)\n",
        "train_df[\"erano\"] = train_df[\"era\"].str.slice(3).astype(int)\n",
        "\n",
        "valid_df = tourney_df[tourney_df['data_type']=='validation']\n",
        "valid_df[\"erano\"] = valid_df[\"era\"].str.slice(3).astype(int)\n",
        "\n",
        "# Extract eras\n",
        "eras = train_df[\"erano\"]\n",
        "target = \"target\"\n",
        "\n",
        "print(\"Training:\", train_df.shape)\n",
        "print(\"Validation:\", valid_df.shape)\n",
        "print(\"Tournament:\", tourney_df.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: (501808, 315)\n",
            "Validation: (137779, 315)\n",
            "Tournament: (1698481, 314)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JE96P57dvwQ",
        "outputId": "0c2592cc-114d-40a0-a3b0-d440dea55a88"
      },
      "source": [
        "print(\"First five features:\")\n",
        "print(features[:5])\n",
        "print()\n",
        "print(\"Number of unique eras in training data: \", len(set(eras)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First five features:\n",
            "['feature_intelligence1', 'feature_intelligence2', 'feature_intelligence3', 'feature_intelligence4', 'feature_intelligence5']\n",
            "\n",
            "Number of unique eras in training data:  120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw3cmBTK6BHY"
      },
      "source": [
        "### Split the training data into training/testing sets\n",
        "Things to think about:\n",
        "* Is it useful to use all of the features?\n",
        "* How should we think about the different eras in the data?\n",
        "* Do some features matter more in particular eras than in other eras?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGkWzEgR5pY-",
        "outputId": "dfb56a6c-0d6b-455f-90a6-94a311372a02"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_df[features], train_df[target],\n",
        "                                                    stratify=train_df['erano'],\n",
        "                                                    test_size=0.25, random_state=0)\n",
        "\n",
        "print(\"X_train size: \", X_train.shape)\n",
        "print(\"X_test_size: \", X_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train size:  (376356, 310)\n",
            "X_test_size:  (125452, 310)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD0lIZabc4tM"
      },
      "source": [
        "Here, I decided to stratify the training dataset based on era ( \"stratify=train_df['erano']\" ). This allows the model to be trained on observations from all eras.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_UkZFcI6jLR"
      },
      "source": [
        "### Dimensionality Reduction Techniques\n",
        "\n",
        "* PCA\n",
        "* K-Means Clustering\n",
        "* Autoencoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uViYWuplkPp4"
      },
      "source": [
        "PCA\n",
        "* You can change the number of components or % variance retained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phzAI9HofgVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "488d1257-fc4b-4af8-bb7e-4a5c82758240"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# You can specify the number of components\n",
        "pca = PCA(n_components=120)\n",
        "pca_train = pca.fit_transform(X_train)\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(np.cumsum(explained_variance))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.10418853 0.18438231 0.23781067 0.27816663 0.31218791 0.34198411\n",
            " 0.36653258 0.38880232 0.40842065 0.42659024 0.44218313 0.45664365\n",
            " 0.47012225 0.48294476 0.49506729 0.50705258 0.51851441 0.52949213\n",
            " 0.54032917 0.55054991 0.56046169 0.57020912 0.57954533 0.58874189\n",
            " 0.59757115 0.60587842 0.61391695 0.62189413 0.62958392 0.63701823\n",
            " 0.64435391 0.65151837 0.65841186 0.66521867 0.67182625 0.67840565\n",
            " 0.68468038 0.69087394 0.69692735 0.70279109 0.70852448 0.71415297\n",
            " 0.71968002 0.72509362 0.73040029 0.73557974 0.74063086 0.7455087\n",
            " 0.75029002 0.7550304  0.75968773 0.76425439 0.76874844 0.77320189\n",
            " 0.77752846 0.78179288 0.78598475 0.79013675 0.79415591 0.79816101\n",
            " 0.80210181 0.8059548  0.80971857 0.81343191 0.817077   0.82065529\n",
            " 0.82411799 0.82754494 0.83093963 0.83425688 0.83753016 0.840792\n",
            " 0.84399386 0.84716242 0.85028036 0.85329643 0.85628105 0.85919797\n",
            " 0.86204967 0.86481714 0.86754756 0.87024045 0.87286877 0.87547882\n",
            " 0.87806247 0.88060077 0.88309165 0.88553618 0.88794799 0.89028632\n",
            " 0.89259657 0.89475927 0.89689408 0.89896474 0.90100026 0.90300202\n",
            " 0.90496873 0.90690904 0.9088154  0.91067653 0.91244412 0.91419758\n",
            " 0.9159462  0.91765019 0.91930232 0.92093191 0.9225054  0.92406423\n",
            " 0.92558312 0.92706226 0.92851413 0.92993248 0.93132839 0.93270698\n",
            " 0.93406554 0.93541088 0.93671048 0.93797926 0.9392379  0.94048687]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MstCOZRqfgKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d34259-e8ed-4d23-95cf-b671d04e7979"
      },
      "source": [
        "# You could also specify the % of variance you want to retain\n",
        "pca = PCA(n_components=0.90)\n",
        "pca_train = pca.fit_transform(X_train)\n",
        "\n",
        "# Explained variance of each component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"Number of components with % variance: \", len(explained_variance))\n",
        "print(np.cumsum(explained_variance))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of components with % variance:  95\n",
            "[0.10418853 0.18438231 0.23781067 0.27816663 0.31218791 0.34198411\n",
            " 0.36653258 0.38880232 0.40842065 0.42659024 0.44218313 0.45664365\n",
            " 0.47012225 0.48294476 0.49506729 0.50705258 0.51851441 0.52949213\n",
            " 0.54032917 0.55054991 0.56046169 0.57020912 0.57954533 0.58874189\n",
            " 0.59757115 0.60587842 0.61391695 0.62189414 0.62958392 0.63701823\n",
            " 0.64435391 0.65151837 0.65841186 0.66521867 0.67182625 0.67840565\n",
            " 0.68468038 0.69087394 0.69692735 0.7027911  0.70852448 0.71415298\n",
            " 0.71968003 0.72509363 0.7304003  0.73557974 0.74063087 0.7455087\n",
            " 0.75029003 0.75503041 0.75968774 0.76425441 0.76874846 0.77320192\n",
            " 0.77752848 0.78179291 0.78598479 0.79013678 0.79415595 0.79816105\n",
            " 0.80210185 0.80595484 0.80971862 0.81343196 0.81707707 0.82065537\n",
            " 0.82411808 0.82754506 0.83093977 0.83425703 0.83753036 0.84079221\n",
            " 0.84399408 0.84716267 0.85028064 0.85329674 0.85628139 0.85919834\n",
            " 0.86205015 0.86481775 0.86754834 0.87024136 0.87286978 0.87547995\n",
            " 0.87806388 0.88060228 0.88309324 0.88553807 0.88795012 0.89028855\n",
            " 0.89259925 0.8947623  0.89689735 0.89896888 0.9010064 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHEMSeWOnBd0",
        "outputId": "2329fcd2-d5cd-45b1-a528-870936d28d1d"
      },
      "source": [
        "print(\"Original X_train shape:\", X_train.shape)\n",
        "print(\"PCA Transformed X_train shape:\", pca_train.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original X_train shape: (376356, 310)\n",
            "PCA Transformed X_train shape: (376356, 95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ickvZGnskL8O"
      },
      "source": [
        "K-Means Clustering\n",
        "* You can change the number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOQbsTxgkRMT"
      },
      "source": [
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "\n",
        "kmeans120 = MiniBatchKMeans(n_clusters=120, random_state=6).fit(X_train)\n",
        "kmeans120_train = kmeans120.transform(X_train)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJQyAY7km4AH",
        "outputId": "c128b987-4755-4d11-c2bb-f8e2868e556c"
      },
      "source": [
        "print(\"Original X_train shape:\", X_train.shape)\n",
        "print(\"K-Means Clustered X_train shape:\", kmeans120_train.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original X_train shape: (376356, 310)\n",
            "K-Means Clustered X_train shape: (376356, 120)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCDuaREilNu4"
      },
      "source": [
        "Autoencoder\n",
        "* You can change hidden_layer_size and # of epochs to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxM5aX8l6QlL",
        "outputId": "52459d76-d45d-431a-f35a-5a18064d9343"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "def nn_autoencoder(train_data, hidden_layer_size=120, epochs=10):\n",
        "    input_layer = keras.layers.Flatten(input_shape=(train_data.shape[1],))\n",
        "    hidden_layer = keras.layers.Dense(hidden_layer_size, activation='relu')\n",
        "    output_layer = keras.layers.Dense(train_data.shape[1])\n",
        "    model = keras.Sequential([input_layer, hidden_layer, output_layer])\n",
        "    model.compile('sgd', loss=tf.keras.losses.MeanSquaredError())\n",
        "    model.fit(train_data, train_data, epochs=epochs)\n",
        "    model2 = keras.Sequential([input_layer, \n",
        "                          keras.layers.Dense(hidden_layer_size, activation='sigmoid', weights=model.layers[1].get_weights())\n",
        "                          ])\n",
        "    return model2\n",
        "\n",
        "autoencode_reduction = 240\n",
        "autoencoder = nn_autoencoder(X_train, hidden_layer_size=autoencode_reduction, epochs=8)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "11762/11762 [==============================] - 16s 1ms/step - loss: 0.1523\n",
            "Epoch 2/8\n",
            "11762/11762 [==============================] - 16s 1ms/step - loss: 0.0914\n",
            "Epoch 3/8\n",
            "11762/11762 [==============================] - 16s 1ms/step - loss: 0.0760\n",
            "Epoch 4/8\n",
            "11762/11762 [==============================] - 16s 1ms/step - loss: 0.0668\n",
            "Epoch 5/8\n",
            "11762/11762 [==============================] - 16s 1ms/step - loss: 0.0602\n",
            "Epoch 6/8\n",
            "11762/11762 [==============================] - 17s 1ms/step - loss: 0.0553\n",
            "Epoch 7/8\n",
            "11762/11762 [==============================] - 16s 1ms/step - loss: 0.0514\n",
            "Epoch 8/8\n",
            "11762/11762 [==============================] - 16s 1ms/step - loss: 0.0481\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM4GqnQQ60wt",
        "outputId": "2e9b479f-813f-4ca5-dc16-ecb7cb9cde74"
      },
      "source": [
        "autoenc_X_train = autoencoder.predict(X_train)\n",
        "print(\"Original X_train shape:\", X_train.shape)\n",
        "print(\"Autoencoded X_train shape:\", autoenc_X_train.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original X_train shape: (376356, 310)\n",
            "Autoencoded X_train shape: (376356, 240)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95RQV4SC8WQB"
      },
      "source": [
        "### Bayesian Optimization for xgboost\n",
        "* I am going to use the autoencoded training data but you can easily switch it to PCA or K-means clustering\n",
        "* set 'tree_method':'gpu_hist' to train on GPU for faster training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caV5K2XLbIKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96fb0756-223f-4747-fa5a-459f4edf1df5"
      },
      "source": [
        "# Download bayesian-optimization library\n",
        "! pip install bayesian-optimization"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGr3FR0f8In_"
      },
      "source": [
        "import xgboost\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# Convert autoencoded training data to DMatrix for XGBoost\n",
        "dtrain = xgboost.DMatrix(autoenc_X_train, y_train)\n",
        " \n",
        "def bo_tune_xgb(max_depth, gamma, learning_rate, subsample, colsample_bytree, min_child_weight, n_estimators, alpha, eta):\n",
        "    params = {'max_depth': int(max_depth),\n",
        "              'gamma': gamma,\n",
        "              'learning_rate': learning_rate,\n",
        "              'subsample': subsample,\n",
        "              'colsample_bytree': colsample_bytree,\n",
        "              'min_child_weight': min_child_weight,\n",
        "              'n_estimators': int(n_estimators),\n",
        "              'alpha': alpha,\n",
        "              'eta': eta,\n",
        "              'eval_metric': 'rmse',\n",
        "              'tree_method': 'gpu_hist'}\n",
        " \n",
        "    #Cross validating with the specified parameters in 3 folds\n",
        "    cv_result = xgboost.cv(params, dtrain, nfold=3)\n",
        "    #Return the negative RMSE\n",
        "    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcLmno4v8sr-"
      },
      "source": [
        "#### n_iter \n",
        "* controls how many bayesian optimization steps to perform, more steps = more likely to find a good maximization\n",
        "\n",
        "#### init_points \n",
        "* controls how many steps of random exploration you want to perform to help diversify the exploration space\n",
        "\n",
        "#### acquisition functions\n",
        "* decides how to guide the optimization\n",
        "* acq: {'ucb', 'ei', 'poi'}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMLIAiJp8mIY",
        "outputId": "1cc601b0-4b76-4a23-df68-a0598f480183"
      },
      "source": [
        "# Initial Bayesian Optimization Search\n",
        "xgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (4,12),\n",
        "                                             'gamma': (0.1, 1),\n",
        "                                             'subsample' : (0.5, 1),\n",
        "                                             'learning_rate' : (0.0001, 0.01),\n",
        "                                             'colsample_bytree': (0.7, 1),\n",
        "                                             'min_child_weight': (4, 10),\n",
        "                                             'n_estimators':(80, 240),\n",
        "                                             'alpha': (0.1, 1),\n",
        "                                             'eta': (0.1, 0.3)\n",
        "                                            })\n",
        "\n",
        "xgb_bo.maximize(n_iter=5, init_points=5, acq='ei')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   |   alpha   | colsam... |    eta    |   gamma   | learni... | max_depth | min_ch... | n_esti... | subsample |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.2233  \u001b[0m | \u001b[0m 0.5812  \u001b[0m | \u001b[0m 0.7408  \u001b[0m | \u001b[0m 0.241   \u001b[0m | \u001b[0m 0.7541  \u001b[0m | \u001b[0m 0.000309\u001b[0m | \u001b[0m 11.48   \u001b[0m | \u001b[0m 4.3     \u001b[0m | \u001b[0m 145.7   \u001b[0m | \u001b[0m 0.818   \u001b[0m |\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.2233  \u001b[0m | \u001b[95m 0.358   \u001b[0m | \u001b[95m 0.9853  \u001b[0m | \u001b[95m 0.2256  \u001b[0m | \u001b[95m 0.1545  \u001b[0m | \u001b[95m 0.008279\u001b[0m | \u001b[95m 8.535   \u001b[0m | \u001b[95m 8.691   \u001b[0m | \u001b[95m 86.66   \u001b[0m | \u001b[95m 0.8487  \u001b[0m |\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.2233  \u001b[0m | \u001b[0m 0.8644  \u001b[0m | \u001b[0m 0.7567  \u001b[0m | \u001b[0m 0.2783  \u001b[0m | \u001b[0m 0.5698  \u001b[0m | \u001b[0m 0.004954\u001b[0m | \u001b[0m 11.15   \u001b[0m | \u001b[0m 4.514   \u001b[0m | \u001b[0m 223.5   \u001b[0m | \u001b[0m 0.9694  \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.2233  \u001b[0m | \u001b[0m 0.8552  \u001b[0m | \u001b[0m 0.9783  \u001b[0m | \u001b[0m 0.1311  \u001b[0m | \u001b[0m 0.1946  \u001b[0m | \u001b[0m 0.009649\u001b[0m | \u001b[0m 5.148   \u001b[0m | \u001b[0m 7.246   \u001b[0m | \u001b[0m 101.2   \u001b[0m | \u001b[0m 0.6692  \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.2233  \u001b[0m | \u001b[0m 0.8987  \u001b[0m | \u001b[0m 0.7973  \u001b[0m | \u001b[0m 0.1075  \u001b[0m | \u001b[0m 0.7394  \u001b[0m | \u001b[0m 0.009176\u001b[0m | \u001b[0m 8.436   \u001b[0m | \u001b[0m 9.476   \u001b[0m | \u001b[0m 108.1   \u001b[0m | \u001b[0m 0.7568  \u001b[0m |\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.2233  \u001b[0m | \u001b[0m 0.1179  \u001b[0m | \u001b[0m 0.7364  \u001b[0m | \u001b[0m 0.145   \u001b[0m | \u001b[0m 0.6673  \u001b[0m | \u001b[0m 0.002845\u001b[0m | \u001b[0m 5.641   \u001b[0m | \u001b[0m 9.902   \u001b[0m | \u001b[0m 239.9   \u001b[0m | \u001b[0m 0.5104  \u001b[0m |\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.2233  \u001b[0m | \u001b[0m 0.683   \u001b[0m | \u001b[0m 0.9817  \u001b[0m | \u001b[0m 0.1564  \u001b[0m | \u001b[0m 0.6284  \u001b[0m | \u001b[0m 0.009099\u001b[0m | \u001b[0m 4.415   \u001b[0m | \u001b[0m 4.874   \u001b[0m | \u001b[0m 240.0   \u001b[0m | \u001b[0m 0.5696  \u001b[0m |\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.2233  \u001b[0m | \u001b[0m 0.5491  \u001b[0m | \u001b[0m 0.7815  \u001b[0m | \u001b[0m 0.2008  \u001b[0m | \u001b[0m 0.8885  \u001b[0m | \u001b[0m 0.003671\u001b[0m | \u001b[0m 4.239   \u001b[0m | \u001b[0m 5.197   \u001b[0m | \u001b[0m 80.12   \u001b[0m | \u001b[0m 0.9627  \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.2233  \u001b[0m | \u001b[0m 0.9577  \u001b[0m | \u001b[0m 0.8496  \u001b[0m | \u001b[0m 0.1293  \u001b[0m | \u001b[0m 0.2647  \u001b[0m | \u001b[0m 0.009953\u001b[0m | \u001b[0m 10.85   \u001b[0m | \u001b[0m 8.288   \u001b[0m | \u001b[0m 239.8   \u001b[0m | \u001b[0m 0.6101  \u001b[0m |\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.2233  \u001b[0m | \u001b[0m 0.2077  \u001b[0m | \u001b[0m 0.7118  \u001b[0m | \u001b[0m 0.2939  \u001b[0m | \u001b[0m 0.6842  \u001b[0m | \u001b[0m 0.000255\u001b[0m | \u001b[0m 11.09   \u001b[0m | \u001b[0m 9.777   \u001b[0m | \u001b[0m 80.1    \u001b[0m | \u001b[0m 0.528   \u001b[0m |\n",
            "=====================================================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPAb6BLmuRGk"
      },
      "source": [
        "Just for this notebook I kept the number of iterations and initiation points relatively small so it doesn't a long time to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK-8zuhP8_QN",
        "outputId": "afdab18c-5433-4ca1-a32e-9c1c4c177cb6"
      },
      "source": [
        "# Obtain best parameters from bayesian optimization search\n",
        "\n",
        "params = xgb_bo.max['params']\n",
        "#Converting the max_depth and n_estimator values from float to int\n",
        "params['max_depth']= int(round(params['max_depth']))\n",
        "params['n_estimators']= int(round(params['n_estimators']))\n",
        "params"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alpha': 0.3580124930932794,\n",
              " 'colsample_bytree': 0.9852618386784621,\n",
              " 'eta': 0.22559253407743465,\n",
              " 'gamma': 0.15454714169639497,\n",
              " 'learning_rate': 0.008279493643181786,\n",
              " 'max_depth': 9,\n",
              " 'min_child_weight': 8.690537033017463,\n",
              " 'n_estimators': 87,\n",
              " 'subsample': 0.8487037105346853}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eXTChi_Cve4",
        "outputId": "158c1b1b-db29-47a6-8010-5f25c7fd85e9"
      },
      "source": [
        "#Train XGBRegressor with best params from bayesian optimization\n",
        "autoenc_X_train = autoencoder.predict(X_train)\n",
        "xgb_bestparams = xgboost.XGBRegressor(**params, tree_method='gpu_hist').fit(autoenc_X_train, y_train)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[14:55:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPxZGXWTq2MJ"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_QBkGoOq5O8"
      },
      "source": [
        "# The models should be scored based on the rank-correlation (spearman) with the target\n",
        "def numerai_score(y_true, y_pred):\n",
        "    rank_pred = y_pred.groupby(eras).apply(lambda x: x.rank(pct=True, method=\"first\"))\n",
        "    return np.corrcoef(y_true, rank_pred)[0,1]\n",
        "\n",
        "# It can also be convenient while working to evaluate based on the regular (pearson) correlation\n",
        "def correlation_score(y_true, y_pred):\n",
        "    return np.corrcoef(y_true, y_pred)[0,1]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN1KiBPqFQp2",
        "outputId": "b76c4782-d849-4971-fd1f-8bcb2e55ff70"
      },
      "source": [
        "train_preds = xgb_bestparams.predict(autoenc_X_train)\n",
        "print('Training Scores')\n",
        "print('Numerai Score: ', numerai_score(y_train, pd.Series(train_preds)))\n",
        "print('Correlation Score: ', correlation_score(y_train, pd.Series(train_preds)))\n",
        "print()\n",
        "\n",
        "autoenc_X_test = autoencoder.predict(X_test)\n",
        "test_preds = xgb_bestparams.predict(autoenc_X_test)\n",
        "print('Testing Scores')\n",
        "print('Numerai Score: ', numerai_score(y_test, pd.Series(test_preds)))\n",
        "print('Correlation Score: ', correlation_score(y_test, pd.Series(test_preds)))\n",
        "print()\n",
        "\n",
        "autoenc_validation = autoencoder.predict(valid_df[features])\n",
        "validation_preds = xgb_bestparams.predict(autoenc_validation)\n",
        "print('Validation Scores')\n",
        "print('Numerai Score: ', numerai_score(valid_df[target], pd.Series(validation_preds)))\n",
        "print('Correlation Score: ', correlation_score(valid_df[target], pd.Series(validation_preds)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Scores\n",
            "Numerai Score:  0.41326823670909\n",
            "Correlation Score:  0.4638529633347826\n",
            "\n",
            "Testing Scores\n",
            "Numerai Score:  0.03111450341716244\n",
            "Correlation Score:  0.0314354017677558\n",
            "\n",
            "Validation Scores\n",
            "Numerai Score:  0.015602395208498848\n",
            "Correlation Score:  0.017801418534472258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RXO8SeqpcdW"
      },
      "source": [
        "The model clearly overfits the training data because the numerai and correlation scores of the testing and validation sets drop off significantly from the training set scores. This means that the model will most likely not generalize well to the live tournament data. This model might perform well in a couple of rounds, but over time I predict it will generally underperform. \n",
        "\n",
        "Personally, the best models I have made that are performing well on the live tournament typically have validation scores between 0.03-0.05 and slightly overfit the training data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD0bYsH5q1O5"
      },
      "source": [
        "### Predicting the Live Tournament Data\n",
        "* Recall we read in the tournament data at the beginning of the notebook under tourney_df\n",
        "* Make sure the format is correct (id, prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHyp159OFWF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71474dc8-563c-44f0-9398-7a04fe011b28"
      },
      "source": [
        "# Apply autoencoder for dimensionality reduction to the tournament data\n",
        "autoenc_tourney = autoencoder.predict(tourney_df[features])\n",
        "\n",
        "# Use our best xgboost model from bayesian optimization to predict autoencoded tournament data\n",
        "# Avoid RAM issues by splitting tourney data\n",
        "tourney_preds_1 = xgb_bestparams.predict(autoenc_tourney[:1000000])\n",
        "tourney_preds_2 = xgb_bestparams.predict(autoenc_tourney[1000000:])\n",
        "tourney_preds = np.concatenate((tourney_preds_1, tourney_preds_2))\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['id'] = tourney_ids\n",
        "df['prediction'] = tourney_preds\n",
        "print(\"Current round: \", current_round)\n",
        "df.info()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current round:  260\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1698481 entries, 0 to 1698480\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Dtype  \n",
            "---  ------      -----  \n",
            " 0   id          object \n",
            " 1   prediction  float32\n",
            "dtypes: float32(1), object(1)\n",
            "memory usage: 19.4+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "q6RGlHpHr9y6",
        "outputId": "7ae1838c-e4b9-4673-ec73-05aefcdf972f"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>n0003aa52cab36c2</td>\n",
              "      <td>0.496543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>n000920ed083903f</td>\n",
              "      <td>0.492989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>n0038e640522c4a6</td>\n",
              "      <td>0.518021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>n004ac94a87dc54b</td>\n",
              "      <td>0.496860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>n0052fe97ea0c05f</td>\n",
              "      <td>0.496778</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  prediction\n",
              "0  n0003aa52cab36c2    0.496543\n",
              "1  n000920ed083903f    0.492989\n",
              "2  n0038e640522c4a6    0.518021\n",
              "3  n004ac94a87dc54b    0.496860\n",
              "4  n0052fe97ea0c05f    0.496778"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SiWGuvw5xWx"
      },
      "source": [
        "# Save as csv file you can upload on the numerai website\n",
        "df.to_csv(f'round{current_round}_autoenc_xgb_bayesopt_predictions.csv', index=False)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KgdgFVGrtC_"
      },
      "source": [
        "### Documentation and References\n",
        "\n",
        "Numerapi: https://github.com/numerai/numerapi\n",
        "\n",
        "Numerai Examples: https://github.com/numerai/example-scripts\n",
        "\n",
        "Bayesian Optimization: https://github.com/fmfn/BayesianOptimization\n",
        "\n",
        "XGBoost: https://github.com/dmlc/xgboost/tree/master/demo/guide-python"
      ]
    }
  ]
}