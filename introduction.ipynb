{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "introduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO8rsBUFup1/N4/zKvVpmZV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rnop/nmr_tournament/blob/main/introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz1q_3GYLSSm"
      },
      "source": [
        "### Introduction to the Numerai Tournament\n",
        "Numerai is a hedge fund that trades the global markets based on models created by data scientists all over the world. Numerai is unique in that it provides free high-quality financial datasets that are worth millions of dollars to any user wanting to participate in their tournament. Users are able to build their own models on this anonymized and obfuscated dataset, submit their predictions, and follow their investment performance on the live stock market. If users are confident about their models, they are able to stake on them with real money using Numerai's cryptocurrency, Numeraire (NMR).\n",
        "\n",
        "### About this Notebook\n",
        "The purpose of this notebook is to provide an introduction on how to approach the main Numerai tournament. Alot of the code is taken from example scripts from Numerai's official GitHub here: https://github.com/numerai/example-scripts\n",
        "\n",
        "What's included:\n",
        "* how to read in the Numerai data via API\n",
        "* approaches to dimensionality reduction\n",
        "* training an xgboost model \n",
        "* bayesian optimization techniques\n",
        "* calculating predictions from the current round\n",
        "\n",
        "### Disclaimer\n",
        "**This model is not guaranteed to make you money.** I am currently not staking this particular model in the tournament. This notebook only serves to provide you an introduction to the tournament and to give some of my personal input on how to tackle this data science problem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSJ50dNE3iYR",
        "outputId": "b03d6f1d-b8cb-4719-9534-c2c2fcd66d35"
      },
      "source": [
        "# Download the numerai library \n",
        "! pip install numerapi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numerapi\n",
            "  Downloading https://files.pythonhosted.org/packages/32/7e/b214ea494c316efc2d187df6bff2b1e1e64f921cb522677ff7f145a09ec6/numerapi-2.5.1-py3-none-any.whl\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (1.1.5)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from numerapi) (4.41.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (7.1.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from numerapi) (2018.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->numerapi) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->numerapi) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (3.0.4)\n",
            "Installing collected packages: numerapi\n",
            "Successfully installed numerapi-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9oubZY336Lv"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numerapi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7ziKsCr4NBy"
      },
      "source": [
        "### Import data\n",
        "* Training data contains 501,808 observations\n",
        "* Tournament data contains the testing and validation sets, and the live observations you need to predict on for the upcoming round"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSm76usa4KtR",
        "outputId": "7a884fbc-9c5a-429a-fb1e-932e4971ccdc"
      },
      "source": [
        "host = 'numerai-public-datasets.s3-us-west-2.amazonaws.com'\n",
        "train_filename = 'latest_numerai_training_data.csv.xz'\n",
        "tourney_filename = 'latest_numerai_tournament_data.csv.xz'\n",
        "\n",
        "train_df = pd.read_csv('https://{}/{}'.format(host, train_filename))\n",
        "tourney_df = pd.read_csv('https://{}/{}'.format(host, tourney_filename))\n",
        "\n",
        "#Confirm round number\n",
        "napi = numerapi.NumerAPI(verbosity=\"info\")\n",
        "current_round = napi.get_current_round()\n",
        "print()\n",
        "print(\"ROUND NUMBER: \", current_round)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "ROUND NUMBER:  267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "EcpHlJib4Tg4",
        "outputId": "608e0078-09af-4307-9cea-67044a8f8413"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>era</th>\n",
              "      <th>data_type</th>\n",
              "      <th>feature_intelligence1</th>\n",
              "      <th>feature_intelligence2</th>\n",
              "      <th>feature_intelligence3</th>\n",
              "      <th>feature_intelligence4</th>\n",
              "      <th>feature_intelligence5</th>\n",
              "      <th>feature_intelligence6</th>\n",
              "      <th>feature_intelligence7</th>\n",
              "      <th>feature_intelligence8</th>\n",
              "      <th>feature_intelligence9</th>\n",
              "      <th>feature_intelligence10</th>\n",
              "      <th>feature_intelligence11</th>\n",
              "      <th>feature_intelligence12</th>\n",
              "      <th>feature_charisma1</th>\n",
              "      <th>feature_charisma2</th>\n",
              "      <th>feature_charisma3</th>\n",
              "      <th>feature_charisma4</th>\n",
              "      <th>feature_charisma5</th>\n",
              "      <th>feature_charisma6</th>\n",
              "      <th>feature_charisma7</th>\n",
              "      <th>feature_charisma8</th>\n",
              "      <th>feature_charisma9</th>\n",
              "      <th>feature_charisma10</th>\n",
              "      <th>feature_charisma11</th>\n",
              "      <th>feature_charisma12</th>\n",
              "      <th>feature_charisma13</th>\n",
              "      <th>feature_charisma14</th>\n",
              "      <th>feature_charisma15</th>\n",
              "      <th>feature_charisma16</th>\n",
              "      <th>feature_charisma17</th>\n",
              "      <th>feature_charisma18</th>\n",
              "      <th>feature_charisma19</th>\n",
              "      <th>feature_charisma20</th>\n",
              "      <th>feature_charisma21</th>\n",
              "      <th>feature_charisma22</th>\n",
              "      <th>feature_charisma23</th>\n",
              "      <th>feature_charisma24</th>\n",
              "      <th>feature_charisma25</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_wisdom8</th>\n",
              "      <th>feature_wisdom9</th>\n",
              "      <th>feature_wisdom10</th>\n",
              "      <th>feature_wisdom11</th>\n",
              "      <th>feature_wisdom12</th>\n",
              "      <th>feature_wisdom13</th>\n",
              "      <th>feature_wisdom14</th>\n",
              "      <th>feature_wisdom15</th>\n",
              "      <th>feature_wisdom16</th>\n",
              "      <th>feature_wisdom17</th>\n",
              "      <th>feature_wisdom18</th>\n",
              "      <th>feature_wisdom19</th>\n",
              "      <th>feature_wisdom20</th>\n",
              "      <th>feature_wisdom21</th>\n",
              "      <th>feature_wisdom22</th>\n",
              "      <th>feature_wisdom23</th>\n",
              "      <th>feature_wisdom24</th>\n",
              "      <th>feature_wisdom25</th>\n",
              "      <th>feature_wisdom26</th>\n",
              "      <th>feature_wisdom27</th>\n",
              "      <th>feature_wisdom28</th>\n",
              "      <th>feature_wisdom29</th>\n",
              "      <th>feature_wisdom30</th>\n",
              "      <th>feature_wisdom31</th>\n",
              "      <th>feature_wisdom32</th>\n",
              "      <th>feature_wisdom33</th>\n",
              "      <th>feature_wisdom34</th>\n",
              "      <th>feature_wisdom35</th>\n",
              "      <th>feature_wisdom36</th>\n",
              "      <th>feature_wisdom37</th>\n",
              "      <th>feature_wisdom38</th>\n",
              "      <th>feature_wisdom39</th>\n",
              "      <th>feature_wisdom40</th>\n",
              "      <th>feature_wisdom41</th>\n",
              "      <th>feature_wisdom42</th>\n",
              "      <th>feature_wisdom43</th>\n",
              "      <th>feature_wisdom44</th>\n",
              "      <th>feature_wisdom45</th>\n",
              "      <th>feature_wisdom46</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>n000315175b67977</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>n0014af834a96cdd</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>n001c93979ac41d4</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>n0034e4143f22a13</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>n00679d1a636062f</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 314 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id   era data_type  ...  feature_wisdom45  feature_wisdom46  target\n",
              "0  n000315175b67977  era1     train  ...              0.50              0.75    0.50\n",
              "1  n0014af834a96cdd  era1     train  ...              0.25              1.00    0.25\n",
              "2  n001c93979ac41d4  era1     train  ...              0.25              0.75    0.25\n",
              "3  n0034e4143f22a13  era1     train  ...              1.00              1.00    0.25\n",
              "4  n00679d1a636062f  era1     train  ...              0.25              0.75    0.75\n",
              "\n",
              "[5 rows x 314 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "Skw9H8Sd4xJs",
        "outputId": "94b26290-5cf2-47bc-e61b-450d707b909d"
      },
      "source": [
        "tourney_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>era</th>\n",
              "      <th>data_type</th>\n",
              "      <th>feature_intelligence1</th>\n",
              "      <th>feature_intelligence2</th>\n",
              "      <th>feature_intelligence3</th>\n",
              "      <th>feature_intelligence4</th>\n",
              "      <th>feature_intelligence5</th>\n",
              "      <th>feature_intelligence6</th>\n",
              "      <th>feature_intelligence7</th>\n",
              "      <th>feature_intelligence8</th>\n",
              "      <th>feature_intelligence9</th>\n",
              "      <th>feature_intelligence10</th>\n",
              "      <th>feature_intelligence11</th>\n",
              "      <th>feature_intelligence12</th>\n",
              "      <th>feature_charisma1</th>\n",
              "      <th>feature_charisma2</th>\n",
              "      <th>feature_charisma3</th>\n",
              "      <th>feature_charisma4</th>\n",
              "      <th>feature_charisma5</th>\n",
              "      <th>feature_charisma6</th>\n",
              "      <th>feature_charisma7</th>\n",
              "      <th>feature_charisma8</th>\n",
              "      <th>feature_charisma9</th>\n",
              "      <th>feature_charisma10</th>\n",
              "      <th>feature_charisma11</th>\n",
              "      <th>feature_charisma12</th>\n",
              "      <th>feature_charisma13</th>\n",
              "      <th>feature_charisma14</th>\n",
              "      <th>feature_charisma15</th>\n",
              "      <th>feature_charisma16</th>\n",
              "      <th>feature_charisma17</th>\n",
              "      <th>feature_charisma18</th>\n",
              "      <th>feature_charisma19</th>\n",
              "      <th>feature_charisma20</th>\n",
              "      <th>feature_charisma21</th>\n",
              "      <th>feature_charisma22</th>\n",
              "      <th>feature_charisma23</th>\n",
              "      <th>feature_charisma24</th>\n",
              "      <th>feature_charisma25</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_wisdom8</th>\n",
              "      <th>feature_wisdom9</th>\n",
              "      <th>feature_wisdom10</th>\n",
              "      <th>feature_wisdom11</th>\n",
              "      <th>feature_wisdom12</th>\n",
              "      <th>feature_wisdom13</th>\n",
              "      <th>feature_wisdom14</th>\n",
              "      <th>feature_wisdom15</th>\n",
              "      <th>feature_wisdom16</th>\n",
              "      <th>feature_wisdom17</th>\n",
              "      <th>feature_wisdom18</th>\n",
              "      <th>feature_wisdom19</th>\n",
              "      <th>feature_wisdom20</th>\n",
              "      <th>feature_wisdom21</th>\n",
              "      <th>feature_wisdom22</th>\n",
              "      <th>feature_wisdom23</th>\n",
              "      <th>feature_wisdom24</th>\n",
              "      <th>feature_wisdom25</th>\n",
              "      <th>feature_wisdom26</th>\n",
              "      <th>feature_wisdom27</th>\n",
              "      <th>feature_wisdom28</th>\n",
              "      <th>feature_wisdom29</th>\n",
              "      <th>feature_wisdom30</th>\n",
              "      <th>feature_wisdom31</th>\n",
              "      <th>feature_wisdom32</th>\n",
              "      <th>feature_wisdom33</th>\n",
              "      <th>feature_wisdom34</th>\n",
              "      <th>feature_wisdom35</th>\n",
              "      <th>feature_wisdom36</th>\n",
              "      <th>feature_wisdom37</th>\n",
              "      <th>feature_wisdom38</th>\n",
              "      <th>feature_wisdom39</th>\n",
              "      <th>feature_wisdom40</th>\n",
              "      <th>feature_wisdom41</th>\n",
              "      <th>feature_wisdom42</th>\n",
              "      <th>feature_wisdom43</th>\n",
              "      <th>feature_wisdom44</th>\n",
              "      <th>feature_wisdom45</th>\n",
              "      <th>feature_wisdom46</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>n0003aa52cab36c2</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>n000920ed083903f</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>n0038e640522c4a6</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>n004ac94a87dc54b</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>n0052fe97ea0c05f</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 314 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id     era  ... feature_wisdom46  target\n",
              "0  n0003aa52cab36c2  era121  ...             0.00    0.25\n",
              "1  n000920ed083903f  era121  ...             0.50    0.50\n",
              "2  n0038e640522c4a6  era121  ...             0.00    1.00\n",
              "3  n004ac94a87dc54b  era121  ...             0.25    0.50\n",
              "4  n0052fe97ea0c05f  era121  ...             1.00    0.75\n",
              "\n",
              "[5 rows x 314 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y8F2NgN4400",
        "outputId": "a37e409d-5008-458b-fe96-c3443d0310c6"
      },
      "source": [
        "# Number of observations in each dataset type\n",
        "tourney_df['data_type'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "test          1593237\n",
              "validation     137779\n",
              "live             5389\n",
              "Name: data_type, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu9JcWlX5cnK"
      },
      "source": [
        "### Data Preprocessing\n",
        "Most of the data cleaning has been done by Numerai in order to anonymize and obfuscate the data to us. This is done purposefully because of the data sharing rights from the data vendors Numerai spends millions of dollars on (thank you Numerai!). \n",
        "\n",
        "Steps:\n",
        "* Extract features\n",
        "* Convert era from string to integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-0pxeCF5Cjd",
        "outputId": "9d0113fc-654b-4487-e6fa-059f90961d3a"
      },
      "source": [
        "# Extract features\n",
        "tourney_ids = tourney_df['id']\n",
        "features = [c for c in tourney_df if c.startswith('feature')]\n",
        "\n",
        "# The training data is also grouped into 120 different eras (1-120)\n",
        "train_df[\"erano\"] = train_df[\"era\"].str.slice(3).astype(int)\n",
        "\n",
        "valid_df = tourney_df[tourney_df['data_type']=='validation']\n",
        "valid_df[\"erano\"] = valid_df[\"era\"].str.slice(3).astype(int)\n",
        "\n",
        "# Extract eras\n",
        "eras = train_df[\"erano\"]\n",
        "target = \"target\"\n",
        "\n",
        "print(\"Training:\", train_df.shape)\n",
        "print(\"Validation:\", valid_df.shape)\n",
        "print(\"Tournament:\", tourney_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: (501808, 315)\n",
            "Validation: (137779, 315)\n",
            "Tournament: (1736405, 314)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JE96P57dvwQ",
        "outputId": "9fcadd60-a58a-4e3b-cf05-db3ad1fad871"
      },
      "source": [
        "print(\"First five features:\")\n",
        "print(features[:5])\n",
        "print()\n",
        "print(\"Number of unique eras in training data: \", len(set(eras)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First five features:\n",
            "['feature_intelligence1', 'feature_intelligence2', 'feature_intelligence3', 'feature_intelligence4', 'feature_intelligence5']\n",
            "\n",
            "Number of unique eras in training data:  120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw3cmBTK6BHY"
      },
      "source": [
        "### Split the training data into training/testing sets\n",
        "Things to think about:\n",
        "* Is it useful to use all of the features?\n",
        "* How should we think about the different eras in the data?\n",
        "* Do some features matter more in particular eras than in other eras?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGkWzEgR5pY-",
        "outputId": "43cadcdf-2c1c-4fe7-d32f-e30d844f8c92"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_df[features], train_df[target],\n",
        "                                                    test_size=0.25, random_state=0)\n",
        "\n",
        "print(\"X_train size: \", X_train.shape)\n",
        "print(\"X_test_size: \", X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train size:  (376356, 310)\n",
            "X_test_size:  (125452, 310)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_UkZFcI6jLR"
      },
      "source": [
        "### Dimensionality Reduction Techniques\n",
        "The original dataset has 310 features so it might be useful to reduce the number of features. Here I will implement the following dimension reduction techniques:\n",
        "* PCA\n",
        "* K-Means Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uViYWuplkPp4"
      },
      "source": [
        "PCA\n",
        "* You can change the number of components or % variance retained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phzAI9HofgVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b279421-a39d-4dd5-9dd1-86fa4ec4ca46"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# You can specify the number of components\n",
        "pca = PCA(n_components=120)\n",
        "pca_train = pca.fit_transform(X_train)\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(np.cumsum(explained_variance))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.10420083 0.18434788 0.23781185 0.27809144 0.31211651 0.34185984\n",
            " 0.36638192 0.38865699 0.40826849 0.42644541 0.44202931 0.45648403\n",
            " 0.46998241 0.48279143 0.49494177 0.50692192 0.51838483 0.5293772\n",
            " 0.54021484 0.55043026 0.56035963 0.57011853 0.57947186 0.58864781\n",
            " 0.59747686 0.60579155 0.61383015 0.62180926 0.6294822  0.6369286\n",
            " 0.64428767 0.65146668 0.65836099 0.66518295 0.67180242 0.67836877\n",
            " 0.68466644 0.69086353 0.69690065 0.70276687 0.70850262 0.71413115\n",
            " 0.71965471 0.72507904 0.73038158 0.73555205 0.74058998 0.74546826\n",
            " 0.75023816 0.75497613 0.75963509 0.76420082 0.76869342 0.77315534\n",
            " 0.77747269 0.78173191 0.78592465 0.79006857 0.79408329 0.79808811\n",
            " 0.80202667 0.80588437 0.8096537  0.81335562 0.81699491 0.82057577\n",
            " 0.82404153 0.82747084 0.83087077 0.83419196 0.83745956 0.84072437\n",
            " 0.84393026 0.84710119 0.85022973 0.85324693 0.85624378 0.85916327\n",
            " 0.86201852 0.86478824 0.86752505 0.87021991 0.87284845 0.87545582\n",
            " 0.87804558 0.88059169 0.88307603 0.88551711 0.88792849 0.8902743\n",
            " 0.89258281 0.89474379 0.89687605 0.89894653 0.90098076 0.90297819\n",
            " 0.90494415 0.90688917 0.90879596 0.91066028 0.91242487 0.91418168\n",
            " 0.91592358 0.91762814 0.9192848  0.92091202 0.9224847  0.92404372\n",
            " 0.92555842 0.9270339  0.92848953 0.92990586 0.93130135 0.93268214\n",
            " 0.93404157 0.93537218 0.93668676 0.93797015 0.9392273  0.94044126]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MstCOZRqfgKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ed86b9-4b97-498b-edc4-836cdd483e5d"
      },
      "source": [
        "# You could also specify the % of variance you want to retain\n",
        "pca = PCA(n_components=0.90) #specify 90% retention \n",
        "pca_train = pca.fit_transform(X_train)\n",
        "\n",
        "# Explained variance of each component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"Number of components with % variance: \", len(explained_variance))\n",
        "print(np.cumsum(explained_variance))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of components with % variance:  95\n",
            "[0.10420083 0.18434788 0.23781185 0.27809144 0.31211651 0.34185984\n",
            " 0.36638192 0.38865699 0.40826849 0.42644541 0.44202931 0.45648403\n",
            " 0.46998241 0.48279143 0.49494177 0.50692192 0.51838483 0.5293772\n",
            " 0.54021484 0.55043026 0.56035963 0.57011853 0.57947186 0.58864781\n",
            " 0.59747686 0.60579155 0.61383015 0.62180926 0.6294822  0.6369286\n",
            " 0.64428767 0.65146668 0.65836099 0.66518296 0.67180242 0.67836877\n",
            " 0.68466644 0.69086353 0.69690065 0.70276688 0.70850262 0.71413116\n",
            " 0.71965472 0.72507905 0.73038159 0.73555206 0.74059    0.74546827\n",
            " 0.75023817 0.75497615 0.75963511 0.76420085 0.76869344 0.77315537\n",
            " 0.77747272 0.78173194 0.7859247  0.79006862 0.79408334 0.79808818\n",
            " 0.80202674 0.80588445 0.80965381 0.81335573 0.81699503 0.82057589\n",
            " 0.8240417  0.82747103 0.83087104 0.83419225 0.83745992 0.84072477\n",
            " 0.84393068 0.84710162 0.85023017 0.85324781 0.8562447  0.85916427\n",
            " 0.86201979 0.86478959 0.86752645 0.87022146 0.87285012 0.87545754\n",
            " 0.87804736 0.88059364 0.88307843 0.88551967 0.8879315  0.89027869\n",
            " 0.89258786 0.89474929 0.89688249 0.89895507 0.90099071]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHEMSeWOnBd0",
        "outputId": "15ef5640-44cf-4bc6-c381-9dc15f148e14"
      },
      "source": [
        "print(\"Original X_train shape:\", X_train.shape)\n",
        "print(\"PCA Transformed X_train shape:\", pca_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original X_train shape: (376356, 310)\n",
            "PCA Transformed X_train shape: (376356, 95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ickvZGnskL8O"
      },
      "source": [
        "K-Means Clustering\n",
        "* You can change the number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOQbsTxgkRMT"
      },
      "source": [
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "\n",
        "# Reduce to 120 clusters\n",
        "kmeans120 = MiniBatchKMeans(n_clusters=120, random_state=6).fit(X_train)\n",
        "kmeans120_train = kmeans120.transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJQyAY7km4AH",
        "outputId": "a9e9254a-ce45-41fc-ed06-661b6b8df06f"
      },
      "source": [
        "print(\"Original X_train shape:\", X_train.shape)\n",
        "print(\"K-Means Clustered X_train shape:\", kmeans120_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original X_train shape: (376356, 310)\n",
            "K-Means Clustered X_train shape: (376356, 120)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95RQV4SC8WQB"
      },
      "source": [
        "### Bayesian Optimization for xgboost\n",
        "Documentation: https://github.com/fmfn/BayesianOptimization\n",
        "* I am going to use the PCA transformed training data but you can easily switch it to PCA or K-means clustering\n",
        "* set 'tree_method':'gpu_hist' to train on GPU for faster training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caV5K2XLbIKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b44bbea-d075-4dfb-f0b0-e79e66f78877"
      },
      "source": [
        "# Download bayesian-optimization library\n",
        "! pip install bayesian-optimization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7a/fd8059a3881d3ab37ac8f72f56b73937a14e8bb14a9733e68cc8b17dbe3c/bayesian-optimization-1.2.0.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.0.1)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-cp37-none-any.whl size=11686 sha256=1ef86227adf191a6a01d7a3e3cf1077afcf3cf5137d2fc728114bb6269081eeb\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/56/ae/e0e3c1fc1954dc3ec712e2df547235ed072b448094d8f94aec\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGr3FR0f8In_"
      },
      "source": [
        "import xgboost\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# Convert autoencoded training data to DMatrix for XGBoost\n",
        "dtrain = xgboost.DMatrix(pca_train, y_train)\n",
        " \n",
        "def bo_tune_xgb(max_depth, gamma, learning_rate, subsample, colsample_bytree, min_child_weight, n_estimators, alpha, eta):\n",
        "    params = {'max_depth': int(max_depth),\n",
        "              'gamma': gamma,\n",
        "              'learning_rate': learning_rate,\n",
        "              'subsample': subsample,\n",
        "              'colsample_bytree': colsample_bytree,\n",
        "              'min_child_weight': min_child_weight,\n",
        "              'n_estimators': int(n_estimators),\n",
        "              'alpha': alpha,\n",
        "              'eta': eta,\n",
        "              'eval_metric': 'rmse',\n",
        "              'tree_method': 'gpu_hist'}\n",
        " \n",
        "    #Cross validating with the specified parameters in 3 folds\n",
        "    cv_result = xgboost.cv(params, dtrain, nfold=3)\n",
        "    #Return the negative RMSE\n",
        "    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcLmno4v8sr-"
      },
      "source": [
        "#### n_iter \n",
        "* controls how many bayesian optimization steps to perform, more steps = more likely to find a good maximization\n",
        "\n",
        "#### init_points \n",
        "* controls how many steps of random exploration you want to perform to help diversify the exploration space\n",
        "\n",
        "#### acquisition functions\n",
        "* decides how to guide the optimization\n",
        "* acq: {'ucb', 'ei', 'poi'}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMLIAiJp8mIY",
        "outputId": "d21475c9-888b-464a-8c04-0441d9ab3cdd"
      },
      "source": [
        "# Initial Bayesian Optimization Search\n",
        "xgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (4,12),\n",
        "                                             'gamma': (0.1, 1),\n",
        "                                             'subsample' : (0.5, 1),\n",
        "                                             'learning_rate' : (0.0001, 0.01),\n",
        "                                             'colsample_bytree': (0.7, 1),\n",
        "                                             'min_child_weight': (4, 10),\n",
        "                                             'n_estimators':(80, 240),\n",
        "                                             'alpha': (0.1, 1),\n",
        "                                             'eta': (0.1, 0.3)\n",
        "                                            })\n",
        "\n",
        "xgb_bo.maximize(n_iter=5, init_points=5, acq='ei')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   |   alpha   | colsam... |    eta    |   gamma   | learni... | max_depth | min_ch... | n_esti... | subsample |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.2235  \u001b[0m | \u001b[0m 0.7098  \u001b[0m | \u001b[0m 0.7666  \u001b[0m | \u001b[0m 0.2587  \u001b[0m | \u001b[0m 0.2934  \u001b[0m | \u001b[0m 0.008873\u001b[0m | \u001b[0m 4.542   \u001b[0m | \u001b[0m 4.241   \u001b[0m | \u001b[0m 150.4   \u001b[0m | \u001b[0m 0.6259  \u001b[0m |\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.2235  \u001b[0m | \u001b[0m 0.1985  \u001b[0m | \u001b[0m 0.8442  \u001b[0m | \u001b[0m 0.2617  \u001b[0m | \u001b[0m 0.2992  \u001b[0m | \u001b[0m 0.005382\u001b[0m | \u001b[0m 7.494   \u001b[0m | \u001b[0m 6.304   \u001b[0m | \u001b[0m 80.55   \u001b[0m | \u001b[0m 0.7618  \u001b[0m |\n",
            "| \u001b[95m 3       \u001b[0m | \u001b[95m-0.2235  \u001b[0m | \u001b[95m 0.8231  \u001b[0m | \u001b[95m 0.8734  \u001b[0m | \u001b[95m 0.2961  \u001b[0m | \u001b[95m 0.989   \u001b[0m | \u001b[95m 0.007095\u001b[0m | \u001b[95m 8.833   \u001b[0m | \u001b[95m 9.621   \u001b[0m | \u001b[95m 156.9   \u001b[0m | \u001b[95m 0.741   \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.2235  \u001b[0m | \u001b[0m 0.1972  \u001b[0m | \u001b[0m 0.7617  \u001b[0m | \u001b[0m 0.269   \u001b[0m | \u001b[0m 0.1766  \u001b[0m | \u001b[0m 0.000883\u001b[0m | \u001b[0m 7.799   \u001b[0m | \u001b[0m 5.088   \u001b[0m | \u001b[0m 182.4   \u001b[0m | \u001b[0m 0.9364  \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.2235  \u001b[0m | \u001b[0m 0.7896  \u001b[0m | \u001b[0m 0.8517  \u001b[0m | \u001b[0m 0.161   \u001b[0m | \u001b[0m 0.9932  \u001b[0m | \u001b[0m 0.006897\u001b[0m | \u001b[0m 7.379   \u001b[0m | \u001b[0m 5.38    \u001b[0m | \u001b[0m 180.4   \u001b[0m | \u001b[0m 0.5485  \u001b[0m |\n",
            "| \u001b[95m 6       \u001b[0m | \u001b[95m-0.2235  \u001b[0m | \u001b[95m 0.5222  \u001b[0m | \u001b[95m 0.8321  \u001b[0m | \u001b[95m 0.2949  \u001b[0m | \u001b[95m 0.6578  \u001b[0m | \u001b[95m 0.008039\u001b[0m | \u001b[95m 11.74   \u001b[0m | \u001b[95m 8.943   \u001b[0m | \u001b[95m 239.9   \u001b[0m | \u001b[95m 0.6228  \u001b[0m |\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.2235  \u001b[0m | \u001b[0m 0.62    \u001b[0m | \u001b[0m 0.952   \u001b[0m | \u001b[0m 0.1815  \u001b[0m | \u001b[0m 0.9493  \u001b[0m | \u001b[0m 0.002319\u001b[0m | \u001b[0m 10.74   \u001b[0m | \u001b[0m 9.828   \u001b[0m | \u001b[0m 80.17   \u001b[0m | \u001b[0m 0.8775  \u001b[0m |\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.2235  \u001b[0m | \u001b[0m 0.2048  \u001b[0m | \u001b[0m 0.824   \u001b[0m | \u001b[0m 0.2632  \u001b[0m | \u001b[0m 0.483   \u001b[0m | \u001b[0m 0.007861\u001b[0m | \u001b[0m 4.22    \u001b[0m | \u001b[0m 4.092   \u001b[0m | \u001b[0m 239.9   \u001b[0m | \u001b[0m 0.9399  \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.2235  \u001b[0m | \u001b[0m 0.3627  \u001b[0m | \u001b[0m 0.9136  \u001b[0m | \u001b[0m 0.252   \u001b[0m | \u001b[0m 0.9417  \u001b[0m | \u001b[0m 0.007269\u001b[0m | \u001b[0m 4.101   \u001b[0m | \u001b[0m 6.569   \u001b[0m | \u001b[0m 80.05   \u001b[0m | \u001b[0m 0.5137  \u001b[0m |\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.2235  \u001b[0m | \u001b[0m 0.6864  \u001b[0m | \u001b[0m 0.8426  \u001b[0m | \u001b[0m 0.2379  \u001b[0m | \u001b[0m 0.7623  \u001b[0m | \u001b[0m 0.009664\u001b[0m | \u001b[0m 4.108   \u001b[0m | \u001b[0m 9.803   \u001b[0m | \u001b[0m 239.9   \u001b[0m | \u001b[0m 0.7402  \u001b[0m |\n",
            "=====================================================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPAb6BLmuRGk"
      },
      "source": [
        "Just for this notebook I kept the number of iterations and initiation points relatively small so it doesn't a long time to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK-8zuhP8_QN",
        "outputId": "029c586b-befd-4eaf-b95c-bcdee8b3d675"
      },
      "source": [
        "# Obtain best parameters from bayesian optimization search\n",
        "params = xgb_bo.max['params']\n",
        "\n",
        "#Converting the max_depth and n_estimator values from float to int\n",
        "params['max_depth']= int(round(params['max_depth']))\n",
        "params['n_estimators']= int(round(params['n_estimators']))\n",
        "params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alpha': 0.5222068780852829,\n",
              " 'colsample_bytree': 0.8320839920288065,\n",
              " 'eta': 0.2948570153308374,\n",
              " 'gamma': 0.6577864279261745,\n",
              " 'learning_rate': 0.008039000691274816,\n",
              " 'max_depth': 12,\n",
              " 'min_child_weight': 8.943039866790368,\n",
              " 'n_estimators': 240,\n",
              " 'subsample': 0.6228463170190888}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eXTChi_Cve4",
        "outputId": "84c29072-974f-40ba-813c-1b2ddd932860"
      },
      "source": [
        "#Re-train XGBRegressor with best params found from bayesian optimization\n",
        "pca_X_train = pca.transform(X_train)\n",
        "xgb_bestparams = xgboost.XGBRegressor(**params, tree_method='gpu_hist').fit(pca_X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[18:52:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPxZGXWTq2MJ"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_QBkGoOq5O8"
      },
      "source": [
        "# The models should be scored based on the rank-correlation (spearman) with the target\n",
        "def numerai_score(y_true, y_pred):\n",
        "    rank_pred = y_pred.groupby(eras).apply(lambda x: x.rank(pct=True, method=\"first\"))\n",
        "    return np.corrcoef(y_true, rank_pred)[0,1]\n",
        "\n",
        "# It can also be convenient while working to evaluate based on the regular (pearson) correlation\n",
        "def correlation_score(y_true, y_pred):\n",
        "    return np.corrcoef(y_true, y_pred)[0,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN1KiBPqFQp2",
        "outputId": "050420b1-5aa7-4980-ddb2-632b827b775a"
      },
      "source": [
        "train_preds = xgb_bestparams.predict(pca_X_train)\n",
        "print('Training Scores')\n",
        "print('Numerai Score: ', numerai_score(y_train, pd.Series(train_preds)))\n",
        "print('Correlation Score: ', correlation_score(y_train, pd.Series(train_preds)))\n",
        "print()\n",
        "\n",
        "pca_X_test = pca.transform(X_test)\n",
        "test_preds = xgb_bestparams.predict(pca_X_test)\n",
        "print('Testing Scores')\n",
        "print('Numerai Score: ', numerai_score(y_test, pd.Series(test_preds)))\n",
        "print('Correlation Score: ', correlation_score(y_test, pd.Series(test_preds)))\n",
        "print()\n",
        "\n",
        "pca_validation = pca.transform(valid_df[features])\n",
        "validation_preds = xgb_bestparams.predict(pca_validation)\n",
        "print('Validation Scores')\n",
        "print('Numerai Score: ', numerai_score(valid_df[target], pd.Series(validation_preds)))\n",
        "print('Correlation Score: ', correlation_score(valid_df[target], pd.Series(validation_preds)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Scores\n",
            "Numerai Score:  0.7679931020365646\n",
            "Correlation Score:  0.7855737788356995\n",
            "\n",
            "Testing Scores\n",
            "Numerai Score:  0.027429486157646475\n",
            "Correlation Score:  0.027736188437666797\n",
            "\n",
            "Validation Scores\n",
            "Numerai Score:  0.008213554830582085\n",
            "Correlation Score:  0.009258990768557676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RXO8SeqpcdW"
      },
      "source": [
        "The model clearly overfits the training data because the numerai and correlation scores of the testing and validation sets drop off significantly from the training set scores. This means that the model will most likely not generalize well to the live tournament data. This model might perform well in a couple of rounds, but over time I predict it will generally underperform. \n",
        "\n",
        "Personally, the best models I have made that are performing well on the live tournament typically have validation scores between 0.03-0.05 and slightly overfit the training data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD0bYsH5q1O5"
      },
      "source": [
        "### Predicting the Live Tournament Data\n",
        "* Recall we read in the tournament data at the beginning of the notebook under tourney_df\n",
        "* Make sure the format is correct (id, prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHyp159OFWF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e2d99d-3056-4cfe-8688-123be707b0d6"
      },
      "source": [
        "# Apply autoencoder for dimensionality reduction to the tournament data\n",
        "pca_tourney = pca.transform(tourney_df[features])\n",
        "\n",
        "# Use our best xgboost model from bayesian optimization to predict autoencoded tournament data\n",
        "# Avoid Colab RAM issues by splitting tourney data into two pieces (Usually there are about ~1.5m tournament observations to predict)\n",
        "tourney_preds_1 = xgb_bestparams.predict(pca_tourney[:1000000])\n",
        "tourney_preds_2 = xgb_bestparams.predict(pca_tourney[1000000:])\n",
        "tourney_preds = np.concatenate((tourney_preds_1, tourney_preds_2))\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['id'] = tourney_ids\n",
        "df['prediction'] = tourney_preds\n",
        "print(\"Current round: \", current_round)\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current round:  267\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1736405 entries, 0 to 1736404\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Dtype  \n",
            "---  ------      -----  \n",
            " 0   id          object \n",
            " 1   prediction  float32\n",
            "dtypes: float32(1), object(1)\n",
            "memory usage: 19.9+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "q6RGlHpHr9y6",
        "outputId": "d816283f-6334-415f-f9ce-ab86ef530197"
      },
      "source": [
        "df.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>580018</th>\n",
              "      <td>n5c45ced4ea7d44f</td>\n",
              "      <td>0.498537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>659951</th>\n",
              "      <td>n8288f3646b305f4</td>\n",
              "      <td>0.494559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>615278</th>\n",
              "      <td>n8e08eb479c9a741</td>\n",
              "      <td>0.493599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1302269</th>\n",
              "      <td>n16d9c18e0f16488</td>\n",
              "      <td>0.511629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1592507</th>\n",
              "      <td>n3140c8335f5c58c</td>\n",
              "      <td>0.494623</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       id  prediction\n",
              "580018   n5c45ced4ea7d44f    0.498537\n",
              "659951   n8288f3646b305f4    0.494559\n",
              "615278   n8e08eb479c9a741    0.493599\n",
              "1302269  n16d9c18e0f16488    0.511629\n",
              "1592507  n3140c8335f5c58c    0.494623"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXOAQR3BGj62"
      },
      "source": [
        "Confirm our live predictions are between 0 and 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c2KqvGCGptL",
        "outputId": "a89c00fc-31fc-49a5-da70-4c5d4865032b"
      },
      "source": [
        "df['prediction'].describe().round(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1736405.000\n",
              "mean           0.500\n",
              "std            0.009\n",
              "min            0.435\n",
              "25%            0.495\n",
              "50%            0.500\n",
              "75%            0.505\n",
              "max            0.560\n",
              "Name: prediction, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "6TZm2nn7DeUn",
        "outputId": "3e3abb82-ff84-4ddf-f11c-2da3a14b8c7b"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.displot(df['prediction'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-09 18:53:07,469 INFO numexpr.utils: NumExpr defaulting to 4 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.FacetGrid at 0x7f5339b7c590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAFgCAYAAACFYaNMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df5xcdX3v8dd7dxN+qJigeylNaBNrtDdatRgBRdsKLUTbGtoiP6Rkg8HcVqz1alVo7714tT4eWr2iWIkNJBLUGmmulliRNEYhWgmyAiI/ywoqyUWzEExad0l2Zj73j/lOMq6z2cnuzpxzZt7Px2MeO/M9Z2Y+k+y+9+z3fL/fo4jAzMzaryfrAszMupUD2MwsIw5gM7OMOIDNzDLiADYzy0hf1gW029KlS+Omm27Kugwz6y5q1Nh1R8CPP/541iWYmQFdGMBmZnnhADYzy4gD2MwsIw5gM7OMOIDNzDLiADYzy4gD2MwsIw5gM7OMtCyAJa2TtEvSPePa/0LSA5LulfR3de2XSRqS9KCkM+val6a2IUmX1rUvlHRbav+8pNmt+ixmZq3QyiPga4Gl9Q2SXg0sA14cES8APpzaFwPnAS9Iz7lKUq+kXuATwGuAxcD5aV+ADwJXRMRzgSeBlS38LGZmM65lARwR24Dd45r/HPhAROxL++xK7cuADRGxLyIeAYaAk9JtKCIejoj9wAZgmSQBpwEb0/PXA2e16rOYmbVCu/uAnwe8KnUd3CLpZal9HvBo3X47UttE7c8CfhoRpXHtDUlaJWlQ0uDw8PAMfRQzs+lpdwD3AccCpwDvBK5PR7MtFRFrImJJRCzp7+9v9dtZF4gIxsbG8DUVbTraHcA7gC9E1beBCvBsYCdwQt1+81PbRO1PAHMk9Y1rN2uLUqnEuVfdTKlUmnxnswm0O4D/GXg1gKTnAbOBx4FNwHmSjpC0EFgEfBu4HViURjzMpnqiblNUDzu+DpydXncAuKGtn8S6Xk9v1y2nbTOslcPQPgfcCjxf0g5JK4F1wHPS0LQNwEA6Gr4XuB64D7gJuCQiyqmP9y3AZuB+4Pq0L8C7gbdLGqLaJ7y2VZ/FrF6t+wFg//79jIyMuCvCpkTd9o2zZMmSGBwczLoMK7CxsTFe//Gvop5erv7TE3njtbfx6Te9kmOOOYY2nNKwYvIVMcymq3b0W9/9IIkVa7e7P9gOmwPY7DCUSiXOuXIzpVL5YFdEgNwfbFPgADY7TD29fVTKJSqlEhev+xblSoVKuXSgX9isWQ5gsymqlEtETy/gccE2NQ5gsxkQlbL7ge2wOYDNZoj7ge1wOYDNDsNE3QyVcolSacz9wHZYHMBmh2FsbIyouJ/XZoYD2GyGeCSEHS4HsFmTDox0yLoQ6xgOYLMmlUolVlzzDQ81sxnjADZr0tjYGErjfhvxWGA7XA5gsxkSlTIr1w96LLA1zQFs1oSIYGRkhMpkIyB6en0UbE1zAJtNIiIYHR1tqv83KmUuWPNNHwVbUxzAZpMolUqc94mtNPvj4itlWLMcwGZNaDZUK+WSJ2pY0xzAZmYZcQCbmWXEAWw2gyrlEuVKxSMhrCkOYLNJHG6YeiSENcsBbDbDKuUS0sQz5sxqHMBmk/ASlNYqDmAzs4w4gM1awGsDWzMcwGYt4AC2ZjiAzcwy4gA2OwRfBcNaqWUBLGmdpF2S7mmw7R2SQtKz02NJulLSkKS7JZ1Yt++ApIfSbaCu/aWSvpeec6UkteqzWPfyVTCslVp5BHwtsHR8o6QTgDOAH9U1vwZYlG6rgNVp32OBy4GTgZOAyyXNTc9ZDbyp7nm/8F5mM2Eqq5v56hjWjJYFcERsA3Y32HQF8C74ub/qlgHXRdV2YI6k44EzgS0RsTsingS2AEvTtmMiYntUv8OvA85q1Wex7jXVEPXVMawZbe0DlrQM2BkR3x23aR7waN3jHantUO07GrRP9L6rJA1KGhweHp7GJzBrnrwusE2ibQEs6Wjgr4H/1a73rImINRGxJCKW9Pf3t/vtraCavgyR2RS18wj414CFwHcl/QCYD9wh6ZeAncAJdfvOT22Hap/foN1sxvgEnLVa2wI4Ir4XEf8lIhZExAKq3QYnRsSPgU3A8jQa4hRgT0Q8BmwGzpA0N518OwPYnLbtlXRKGv2wHLihXZ/FuocvL2St1MphaJ8DbgWeL2mHpJWH2P1G4GFgCLgaeDNAROwG3gfcnm7vTW2kfa5Jz/k+8JVWfA4zs1Zp2a/3iDh/ku0L6u4HcMkE+60D1jVoHwReOL0qzVqnNh151qxZWZdiOeWZcGYt4vUgbDIOYDOzjDiAzSYw3Zlsng1nk3EAm7WIZ8PZZBzAZi3k2XB2KA5gswn4WnDWag5gsxbySAg7FAewWQNeiN3awQFs1oDXgbB2cACbTWAm1oFwF4QdigPYzCwjDmCzBjyBwtrBAWw2Tu0E3EycgfNsODsUB7DZOKVSiQvXbJuR0PRsODsUB7BZAzO5EHuAT8RZQw5gM7OMOIDNzDLiADYbxyfNrF0cwGYt5pEQNhEHsFmLRaXMirXbPRLCfoED2KwNvC6wNeIANjPLiAPYzCwjDmCzNvCqaNaIA9jMLCMOYDOzjDiAzcbxxTitXRzAZnV8LThrp5YFsKR1knZJuqeu7UOSHpB0t6QvSppTt+0ySUOSHpR0Zl370tQ2JOnSuvaFkm5L7Z+XNLtVn8W6h68FZ+3UyiPga4Gl49q2AC+MiBcB/w5cBiBpMXAe8IL0nKsk9UrqBT4BvAZYDJyf9gX4IHBFRDwXeBJY2cLPYl1kJpeiNDuUlgVwRGwDdo9r+9eIqM3H3A7MT/eXARsiYl9EPAIMASel21BEPBwR+4ENwDJJAk4DNqbnrwfOatVnMZsuD0OzRrLsA34j8JV0fx7waN22HaltovZnAT+tC/Nae0OSVkkalDQ4PDw8Q+WbmU1PJgEs6W+AEvDZdrxfRKyJiCURsaS/v78db2n2c7wimjXS9gCWtAL4A+CCOPjduBM4oW63+altovYngDmS+sa1m01Lq0LS14azRtoawJKWAu8CXhcRI3WbNgHnSTpC0kJgEfBt4HZgURrxMJvqibpNKbi/Dpydnj8A3NCuz2E2FV4RzcZr5TC0zwG3As+XtEPSSuDvgWcAWyTdJemTABFxL3A9cB9wE3BJRJRTH+9bgM3A/cD1aV+AdwNvlzREtU94bas+i3UPT8KwdmrZr+SIOL9B84QhGRHvB97foP1G4MYG7Q9THSVhZlZIngln1iYeimbjOYDNzDLiADYzy4gD2CzxQjzWbg5gs8QL8Vi7OYDN6rRyIR6fhLPxHMBmbeLpyDaeA9gsaXU4ejqyjecANmsjT0e2eg5gM7OMOIDNknasA+ETcVbPAWxmlhEHsJlZRhzAZmYZcQCb0b5pyB4LbPUcwGa0bxpyVMqsWLvdY4ENcACbHdDKacj1PBbYahzAZrR+FpxZIw5gM7OMOIDNzDLiADZrM8+GsxoHsJlZRhzA1vVqY3N9LSJrNwewdb1SqcSFa7Z5FIS1nQPYjPaNATar5wA2M8uIA9iszTwKwmpaFsCS1knaJemeurZjJW2R9FD6Oje1S9KVkoYk3S3pxLrnDKT9H5I0UNf+UknfS8+5UpJa9VnMZpIX5LGaVh4BXwssHdd2KbA1IhYBW9NjgNcAi9JtFbAaqoENXA6cDJwEXF4L7bTPm+qeN/69zJrS7jD0xTmtpmUBHBHbgN3jmpcB69P99cBZde3XRdV2YI6k44EzgS0RsTsingS2AEvTtmMiYntUf3Kuq3sts9zzgjwG7e8DPi4iHkv3fwwcl+7PAx6t229HajtU+44G7WZmhZHZSbh05NqWv/skrZI0KGlweHi4HW9pBdKOi3GaNdLuAP5J6j4gfd2V2ncCJ9TtNz+1Hap9foP2hiJiTUQsiYgl/f390/4Q1jnadSUMs0baHcCbgNpIhgHghrr25Wk0xCnAntRVsRk4Q9LcdPLtDGBz2rZX0ilp9MPyutcya1q7roQxnoeiGUDLzgRI+hzwO8CzJe2gOprhA8D1klYCPwTOSbvfCLwWGAJGgIsAImK3pPcBt6f93hsRtRN7b6Y60uIo4CvpZnbYenr7cA+EZaFlARwR50+w6fQG+wZwyQSvsw5Y16B9EHjhdGo0M8uSZ8KZmWXEAWyWAc+GM3AAW5fLKgTLY/tYfs2tng3X5RzAZhnxbDhzAJuZZcQBbF3Ns+AsSw5gM7OMOIDNzDLiADbLiKcjmwPYupYX4rGsOYCta2W1EE+Nj4DNAWxdzZejtyw5gM3MMtJUAEs6tZk2syLxWgyWtWaPgD/eZJuZmTXpkB1gkl4OvALol/T2uk3HAL2tLMzMrNNNdgZiNvD0tN8z6tr3Ame3qiizblC/JGX1ylrWbQ4ZwBFxC3CLpGsj4odtqsmsK0SlzMr1g2x86+nMmjUr63IsA82OwTlC0hpgQf1zIuK0VhRl1i28JGV3a/Z//5+ATwLXAOXWlWNm1j2aDeBSRKxuaSVmbVTrf/U8ZMtSs8PQviTpzZKOl3Rs7dbSysxaqFQqceGabR4HbJlq9gh4IH19Z11bAM+Z2XLM2qent49KeX+mNdTWg/BJuO7UVABHxMJWF2Jm1m2aCmBJyxu1R8R1M1uOWXvkZRqyxwJ3t2b7gF9Wd3sV8B7gdS2qyaxrRKXMirXbfXn6LtVsF8Rf1D+WNAfY0JKKzLqMxwJ3r6kuR/kzwP3CVlh5uhqyF2bvXs0uR/klSZvS7cvAg8AXp/qmkv67pHsl3SPpc5KOlLRQ0m2ShiR9XtLstO8R6fFQ2r6g7nUuS+0PSjpzqvWYmWWh2b99Plx3vwT8MCJ2TOUNJc0D3gosjohRSdcD5wGvBa6IiA2SPgmsBFanr09GxHMlnQd8EDhX0uL0vBcAvwx8VdLzIsIz9cysEJo6Ak6L8jxAdUW0ucB0B0/2AUdJ6gOOBh4DTgM2pu3rgbPS/WXpMWn76aqeLl4GbIiIfRHxCDAEnDTNuqwL+GKclhfNdkGcA3wbeD1wDnCbpCktRxkRO6keUf+IavDuAb4D/DQiaqeCdwDz0v15wKPpuaW0/7Pq2xs8Z3z9qyQNShocHh6eStnWQbK+GOd45dIYIyMjuanH2qfZk3B/A7wsIgYiYjnVI83/OZU3lDSX6tHrQqpdB08Dlk7ltZoVEWsiYklELOnv72/lW1kBjI2NoZ78XE+gtiylh6J1n2YDuCcidtU9fuIwnjve7wKPRMRwRIwBXwBOBeakLgmA+cDOdH8ncAJA2v7M9P4H2hs8x6xQPBStOzUbojdJ2ixphaQVwJeBG6f4nj8CTpF0dOrLPR24D/g6B6+yMQDckO5v4uBaFGcDX4vq32qbgPPSKImFwCKq3SRmZoUw2TXhngscFxHvlPTHwCvTpluBz07lDSPiNkkbgTuojqi4E1hDNdQ3SPrb1LY2PWUt8GlJQ8BuqiMfiIh70wiK+9LrXOIREGZWJJP93fNR4DKAiPgC1e4CJP1G2vaHU3nTiLgcuHxc88M0GMUQEU9RPfnX6HXeD7x/KjWYmWVtsi6I4yLie+MbU9uCllRkZtYlJgvgOYfYdtRMFmJm1m0mC+BBSW8a3yjpYqpjd80KJ0/rQFh3m6wP+G3AFyVdwMHAXQLMBv6olYWZdRNfGaM7HTKAI+InwCskvRp4YWr+ckR8reWVmXURr4jWnZpdD/jrVMfpmhVa/ToQvv6EZW2qs9nMCml0dJSBq301ZMsHB7B1nR5P+7WccACbmWXEAWyWA/VXR7bu4QA2ywEvSdmdHMBmOeElKbuPA9gsJzwWuPs4gK2reBqy5YkD2MwsIw5gM7OMOICta/hy9JY3DmDrGnm7HL2ZA9i6Sp6nIXsURPdxAFvX8EwzyxsHsJlZRhzAZjnh9SC6jwPYLCeiUuaiddsZHR3NuhRrEwewWY54PYju4gA2M8uIA9jMLCMOYOsKY2NjhRhj6xNx3cUBbF2hFmx5n4cclTIr1m73wuxdIpMAljRH0kZJD0i6X9LLJR0raYukh9LXuWlfSbpS0pCkuyWdWPc6A2n/hyQNZPFZrBhKpRIXrftWIY4sfSKue2R1BPwx4KaI+HXgxcD9wKXA1ohYBGxNjwFeAyxKt1XAagBJxwKXAycDJwGX10LbrJE8T0Ou5ynJ3aPtASzpmcBvAWsBImJ/RPwUWAasT7utB85K95cB10XVdmCOpOOBM4EtEbE7Ip4EtgBL2/hRrCAigpGRESpeiN1yJosj4IXAMPApSXdKukbS04DjIuKxtM+PgePS/XnAo3XP35HaJmr/BZJWSRqUNDg8PDyDH8WKwKugWV5lEcB9wInA6oj4TeBnHOxuACCqPykz9tMSEWsiYklELOnv75+pl7UCKUr3g3WXLAJ4B7AjIm5LjzdSDeSfpK4F0tddaftO4IS6589PbRO1mxWa+4C7R9sDOCJ+DDwq6fmp6XTgPmATUBvJMADckO5vApan0RCnAHtSV8Vm4AxJc9PJtzNSm5lZIWT1d9lfAJ+VNBt4GLiI6i+D6yWtBH4InJP2vRF4LTAEjKR9iYjdkt4H3J72e29E7G7fR7CiODixQVmX0pT6yRhSMWq2qckkgCPiLmBJg02nN9g3gEsmeJ11wLqZrc4sW1Eps3L9IBvfejqzZs3KuhxrIc+Es443NjZGFGwImidjdAcHsFkO+URcd3AAW0fzpegtzxzA1tE8CcPyzAFsHc+TMCyvHMBmZhlxAFtH8+LmlmcOYLMc8iiI7uAANjPLiAPYLId8bbju4AC2jlbEWXBwcDqyrw3X2RzAZjkV4H7gDucANjPLiAPYzCwjDmDrWPv372dkZMTrQFhuOYCtY5VKJS6+9laPJLDccgBbRyvyOhAeitb5HMBmORWVMsvXfJPR0dGsS7EWcQBbR6odPRa9A9hXxuhsDmDrSKVSiQvXbPOf75ZrDmDrWEXu/63xojydzQFsHalTTl75RFxncwBbRyrqGhDjeU2IzuYANss5n4jrXA5gM7OMOIDNcs4n4jqXA9gs5xzAncsBbB3nwMiBrAsxm0RmASypV9Kdkv4lPV4o6TZJQ5I+L2l2aj8iPR5K2xfUvcZlqf1BSWdm80ksb0qlEiuu+YaHblnuZXkE/JfA/XWPPwhcERHPBZ4EVqb2lcCTqf2KtB+SFgPnAS8AlgJXSeptU+2WY2NjY6jH3wqWf5kEsKT5wO8D16THAk4DNqZd1gNnpfvL0mPS9tPT/suADRGxLyIeAYaAk9rzCczax5MxOldWR8AfBd4FVNLjZwE/jYjaaPMdwLx0fx7wKEDaviftf6C9wXN+jqRVkgYlDQ4PD8/k57Ac6pRJGDWejNG52h7Akv4A2BUR32nXe0bEmohYEhFL+vv72/W2ZjPGF+jsTFlMsTkVeJ2k1wJHAscAHwPmSOpLR7nzgZ1p/53ACcAOSX3AM4En6tpr6p9jZpZ7bT8CjojLImJ+RCygehLtaxFxAfB14Oy02wBwQ7q/KT0mbf9aVDvDNgHnpVESC4FFwLfb9DEspzwEzYokT5PM3w1skPS3wJ3A2tS+Fvi0pCFgN9XQJiLulXQ9cB9QAi6JiHL7y7Y8OTgETSjrYswmkWkAR8TNwM3p/sM0GMUQEU8Br5/g+e8H3t+6Cq1oDgxBK1cm39ksY54JZ1YAno7cmRzAZgXgscCdyQFsHaNTLsTZiMcCdyYHsHWMTr8Qp8cCdx4HsHWUTrgQ50TcDdF5HMDWMTo9nKJSZsXa7e6G6CAOYOsIEcHIyAiVDloDohF3Q3QWB7B1BK8BbEXkALaO0cn9v9aZHMBmBeIJGZ3FAWwdodNPwFlncgBbR+i0Rdgn4qFoncUBbFYgnhHXWRzAVnjdtgawfLKxYziArfA8BM2KygFshddtl6Ev7X+KkZGRrMuwGeAANisYn4jrHA5gK7xuGQFR4xNxncMBbFZAXhOiMziAzcwy4gC2Qtu/fz8jIyNdMwStplwaq35u9wMXmgPYCisiGB0d5eJP3dp1QeR+4M7gALbCql2CqJuGoNXzhIzicwBbYXXb+F/rPA5gs4Ly0pTF5wC2wuq28b/jeUJG8TmAzQoqKmWWr/kmo6OjWZdiU+QAtkLqthXQJuITccXW9gCWdIKkr0u6T9K9kv4ytR8raYukh9LXualdkq6UNCTpbkkn1r3WQNr/IUkD7f4slp29e/cy8A+3dP2f3+4HLrYsjoBLwDsiYjFwCnCJpMXApcDWiFgEbE2PAV4DLEq3VcBqqAY2cDlwMnAScHkttK2z1Y5+8dGfFVzbAzgiHouIO9L9/wDuB+YBy4D1abf1wFnp/jLguqjaDsyRdDxwJrAlInZHxJPAFmBpGz+KZcTr/x7kE3HFlmkfsKQFwG8CtwHHRcRjadOPgePS/XnAo3VP25HaJmpv9D6rJA1KGhweHp6x+i0bHv97UFTKXLRuu0/EFVRmASzp6cD/Bd4WEXvrt0X11/mM/UqPiDURsSQilvT398/Uy1pGun342XheGa24MglgSbOohu9nI+ILqfknqWuB9HVXat8JnFD39PmpbaJ262Ae/fCL3A1RXFmMghCwFrg/Ij5St2kTUBvJMADcUNe+PI2GOAXYk7oqNgNnSJqbTr6dkdqsg42OjjJw9TaHTZ2olHnjtbczOjrqf5eCyeII+FTgQuA0SXel22uBDwC/J+kh4HfTY4AbgYeBIeBq4M0AEbEbeB9we7q9N7VZh6od6fX0ePTDeJVKmQs++Q2vjlYwbf9OjohvAppg8+kN9g/gkgleax2wbuaqszwbHR3lTz32d0KelFE8nglnhdLjkJmQJ2UUjwPYrEM4gIvHAWyFEBGMjIxQ8fCzCXk0RPE4gK0QPPphcr5MUfE4gK0QxsbGkDz7bTKl0hh79uzxL6qCcABb7nnyRfN8FFwsDmDLPXc/HB5PTS4OB7DlmidfHD6PhigOB7DlVkQwOjrKhWt89Hs4PBqiOBzAllulUolzrtxMJSaaOGmNRKXMirW3snfvXodwzjmALdc8822KJF+wswAcwJZLEcGePXsolytZl1JcPb3uisg5B7Dlki87NH3lsX0sv+ZWD0nLMQew5U5t2rEnXkyfh6TlmwPYciUi2Lt3L2/4xFbKFXc/TFe5NMbIyIj/ksgpB7DlSm3NX190c2bULto5MjLi/uAccgBbbtS6HnDXw4yqpL8qzr3qZvcH54wD2HKh1vUw4EkXMy4qZVZ9+jtUKhV3R+SMA9gyV9/vG/K3ZCsEUNr3lEdF5Iy/2y1TtenGF6z+msO3DbxcZb74O94yUzvyPffjX8Xfiu3hS9jni7/rLRM+8s1OpVLmDau3OYRzwN/51na18PVCO9kpje3jnI9v9YI9GXMAW9tEBPv372fPnj3udsiBSqXMn/7DNzxGOENeasraov6ot1wq0zv7SIhy1mV1vdoY4T/7xzu57uJTOeaYY5D8V0m7+BDEWqZ2xLtv3z727NnD6z96E5UQ8hKTuRGVMhdfexvlsTEuvPpbjIyMsG/fPvbv3+8j4jbwT4LNuNoVGcbGxjj/qq9R3r8fenpRT69/qHNIvX1QKVMa28fZV3yFnt4+Zs0+kn/889/iqKOO8hFxCzmAbdrGXwJndHSUgWu+RWnsKYIe6OmtLqxecZdD3qm3D/X2HThJ95n/9iqOOuqo6jaJWbNmOZBnUOEDWNJS4GNAL3BNRHwg45I6VkRQKpXo7e2lVCodCNzaFGL19EKlTCWC3tlHIvmIt8gqlTIXrL754P/prCMOBHItjKG6dnNfX5+DeQoKHcCqLhj7CeD3gB3A7ZI2RcR92VaWX7UQ7eur/tcfztnv0dFRVqy7jdXnv4iV6771c4FbO3JCQg7djlH/f1ofyKEerllxMkcffTQXXfttPvOmUw98T034WhJ9fX2Uy2V6e3spl8tdH9yFDmDgJGAoIh4GkLQBWAbMaAB30oLWIyMjXPgPt7Duja8AYMXV2yiPjUEKU+pCFTjQdVCJoKe3jx6JFWtuoae3j/ofmyiXCKj+cEb83H3gkI+Ltm8ea2rXvrUTqFEpc9HV2w58T/zxh7/0c98r8IvfO7OPOIrrVr2S5Wu2sfail/Om627ns3/22weOpItiJutVkf9ElHQ2sDQiLk6PLwROjoi3jNtvFbAqPXw+8GCLSno28HiLXnsmFaHOItQIxaizCDVCMeqcao2PR8TS8Y1FPwJuSkSsAda0+n0kDUbEkla/z3QVoc4i1AjFqLMINUIx6pzpGos+DngncELd4/mpzcws94oewLcDiyQtlDQbOA/YlHFNZmZNKXQXRESUJL0F2Ex1GNq6iLg3w5Ja3s0xQ4pQZxFqhGLUWYQaoRh1zmiNhT4JZ2ZWZEXvgjAzKywHsJlZRhzATZK0VNKDkoYkXXqI/f5EUkhaMq79VyT9p6S/ymudkl4k6VZJ90r6nqQj81SjpFmS1qfa7pd0WSvqa6ZGSSskDUu6K90urts2IOmhdBtoVY3TqVPSS+r+r++WdG7eaqzbfoykHZL+vlU1TrfO9PP9r+n78j5JC5p604jwbZIb1RN83weeA8wGvgssbrDfM4BtwHZgybhtG4F/Av4qj3VSPSF7N/Di9PhZQG/OanwDsCHdPxr4AbAgixqBFcDfN3juscDD6evcdH9uVv/fh6jzecCidP+XgceAOXmqsW77x4B/PNQ+WdcJ3Az8Xrr/dODoZt7XR8DNOTDlOSL2A7Upz+O9D/gg8FR9o6SzgEeAVo/QmE6dZwB3R8R3ASLiiYiWrJg+nRoDeJqkPuAoYD+wN8MaGzkT2BIRuyPiSWAL8AszoLKuMyL+PSIeSvf/H7AL6M9TjQCSXgocB/xrC2qrN+U6JS0G+iJiC0BE/GdEjDTzXAdwc+YBj9Y93pHaDpB0InBCRHx5XPvTgXcD/7vVRTKNOqkeEYWkzZLukPSuHNa4EfgZ1aO1HwEfjojdWdSY/En6832jpNqEoGafOxOmU+cBkk6ietT3/TzVKKkH+D9AS7vtkun8Wz4P+KmkL0i6U9KHVF0obFIO4BmQvlE+Aryjweb3AFdExH+2tagGJqmzD3glcEH6+keSTm9jecCkNRG8tL8AAAQiSURBVJ4ElKn+ybwQeIek57SxvHpfotr98SKqR7nrM6pjMoesU9LxwKeBiyKikkF9MHGNbwZujIgdGdU13kR19gGvovqL4mVUuzFWNPOCDuDmTDbl+RnAC4GbJf0AOAXYlE4enQz8XWp/G/DXafJI3urcAWyLiMfTn083AifmrMY3ADdFxFhE7AL+DWjF2gGTTnFPXTT70sNrgJc2+9yc1ImkY4AvA38TEdtzWOPLgbek74MPA8sltWq97+nUuQO4K3VflIB/ptmfnVZ1anfSjepvuIepHnXVOuhfcIj9b2bcSbjU/h5aexJuynVSPWF0B9WTW33AV4Hfz1mN7wY+le4/jeqyoy/Kokbg+Lr7fwRsT/ePpdrfPzfdHgGOzer/+xB1zga2Am9r1ffjdGsct88KWnsSbjr/lr1p//70+FPAJc28b6GnIrdLTDDlWdJ7gcGIyMX6E9OpMyKelPQRqutrBNU//cb3wWZaI9XF9z8l6V5AVMP47oxqfKuk1wElYDfpT86I2C3pfVT/HQHeG63pp55WncA5wG8Bz5JUa1sREXflqMa2meb/eVnV4aVbJQn4DnB1M+/rqchmZhlxH7CZWUYcwGZmGXEAm5llxAFsZpYRB7CZWUYcwGbjSPodSf+S7r+u0cpYdfvOkfTmuse/LGljO+q04vMwNOsaknqjiQWGJP0O1Qkzf9DEvguAf4mIF067QOs6PgK2jiBpgaQHJH02rcm6UdLRkn4g6YOS7gBeL+mMtA7uHZL+KS2WVFsL9oG03x/Xve6K2jq0ko6T9EVJ3023VwAfAH4trQ/7oVTHPWn/IyV9StX1i++U9Oq61/yCpJtUXTP479r972X54AC2TvJ84KqI+K9Ul6msdQ08EREnUp1e/T+A302PB4G3q7rw/NXAH1Kd3/9LE7z+lcAtEfFiqnP97wUuBb4fES+JiHeO2/8SICLiN4DzgfU6uMj9S4Bzgd8Azm20Spl1PgewdZJHI+Lf0v3PUF3VDeDz6espwGLg3yTdBQwAvwr8OvBIRDwU1T65z0zw+qcBq6E6/TQi9kxSzytrrxURDwA/pLp0IcDWiNgTEU9RXdPiV5v/mNYpvBaEdZLxJzRqj3+WvorqYunn1+8k6SWtLqyBfXX3y/hnsSv5CNg6ya9Ienm6/wbgm+O2bwdOlfRcAElPk/Q84AFggaRfS/udT2NbgT9Pz+2V9EzgP6guodnIN6iur0x6n18BHjzsT2UdywFsneRB4BJJ91NdCnJ1/caIGKa6gtXnJN0N3Ar8euoGWAV8OZ2E2zXB6/8l8GpJ36O64tXiiHiCapfGPZI+NG7/q4CetP/nqa42tg+zxMPQrCN4OJgVkY+Azcwy4iNgM7OM+AjYzCwjDmAzs4w4gM3MMuIANjPLiAPYzCwj/x/b9FAdLrVOcgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SiWGuvw5xWx"
      },
      "source": [
        "# Save as csv file you can upload on the numerai website\n",
        "df.to_csv(f'round{current_round}_pca_xgb_bayesopt_predictions.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reEpcGC5Ui37"
      },
      "source": [
        "# Alternatively, you can upload the csv directly to Numerai if you don't feel like locally downloading the prediction csv and manually uploading it yourself\n",
        "# I like using this method because I can submit predictions directly from my phone if I am away from the computer on the weekends\n",
        "\n",
        "# import numerapi\n",
        "# napi = numerapi.NumerAPI(\"public_id\", \"secret_key\")\n",
        "\n",
        "# # upload predictions\n",
        "# napi.upload_predictions(\"predictions.csv\", model_id=\"model_id\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KgdgFVGrtC_"
      },
      "source": [
        "### Documentation and References\n",
        "\n",
        "Numerapi: https://github.com/numerai/numerapi\n",
        "\n",
        "Numerai Examples: https://github.com/numerai/example-scripts\n",
        "\n",
        "Bayesian Optimization: https://github.com/fmfn/BayesianOptimization\n",
        "\n",
        "XGBoost: https://github.com/dmlc/xgboost/tree/master/demo/guide-python"
      ]
    }
  ]
}