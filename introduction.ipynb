{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "introduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOhxtz8vYhZv1hELSAIpRbu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rnop/nmr_tournament/blob/main/introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz1q_3GYLSSm"
      },
      "source": [
        "### Introduction to the Numerai Tournament\n",
        "Numerai is a hedge fund that trades the global markets based on models created by data scientists all over the world. Numerai is unique in that it provides free high-quality financial datasets that are worth millions of dollars to any user wanting to participate in their tournament. Users are able to build their own models on this anonymized and obfuscated dataset, submit their predictions, and follow their investment performance on the live stock market. If users are confident about their models, they are able to stake on them with real money using Numerai's cryptocurrency, Numeraire (NMR).\n",
        "\n",
        "### About this Notebook\n",
        "The purpose of this notebook is to provide an introduction on how to approach the main Numerai tournament.\n",
        "\n",
        "What's included:\n",
        "* how to read in the Numerai data via API\n",
        "* approaches to dimensionality reduction\n",
        "* training an xgboost model \n",
        "* bayesian optimization techniques\n",
        "* calculating predictions from the current round\n",
        "\n",
        "### Disclaimer\n",
        "**This model is not guaranteed to make you money.** I am currently not staking this particular model in the tournament. This notebook only serves to provide you an introduction to the tournament and to give some of my personal input on how to tackle this data science problem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSJ50dNE3iYR",
        "outputId": "ffe4aa29-12f3-4cca-9db7-9fc41246350c"
      },
      "source": [
        "# Download the numerai library \n",
        "! pip install numerapi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numerapi\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/96/ebdbaff5a2fef49b212e4f40634166f59e45462a768c0136d148f00255c5/numerapi-2.4.5-py3-none-any.whl\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (7.1.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from numerapi) (2018.9)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from numerapi) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->numerapi) (1.15.0)\n",
            "Installing collected packages: numerapi\n",
            "Successfully installed numerapi-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9oubZY336Lv"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numerapi"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7ziKsCr4NBy"
      },
      "source": [
        "### Import data\n",
        "* Training data contains 501,808 observations\n",
        "* Tournament data contains the testing and validation sets, and the live observations you need to predict on for the upcoming round"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSm76usa4KtR",
        "outputId": "42be3c04-66b8-4459-9f54-a03d5dc53757"
      },
      "source": [
        "host = 'numerai-public-datasets.s3-us-west-2.amazonaws.com'\n",
        "train_filename = 'latest_numerai_training_data.csv.xz'\n",
        "tourney_filename = 'latest_numerai_tournament_data.csv.xz'\n",
        "\n",
        "train_df = pd.read_csv('https://{}/{}'.format(host, train_filename))\n",
        "tourney_df = pd.read_csv('https://{}/{}'.format(host, tourney_filename))\n",
        "\n",
        "#Confirm round number\n",
        "napi = numerapi.NumerAPI(verbosity=\"info\")\n",
        "current_round = napi.get_current_round()\n",
        "print()\n",
        "print(\"ROUND NUMBER: \", current_round)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "ROUND NUMBER:  260\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "EcpHlJib4Tg4",
        "outputId": "293052c0-0530-48a0-9cf7-cca986cf25e9"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>era</th>\n",
              "      <th>data_type</th>\n",
              "      <th>feature_intelligence1</th>\n",
              "      <th>feature_intelligence2</th>\n",
              "      <th>feature_intelligence3</th>\n",
              "      <th>feature_intelligence4</th>\n",
              "      <th>feature_intelligence5</th>\n",
              "      <th>feature_intelligence6</th>\n",
              "      <th>feature_intelligence7</th>\n",
              "      <th>feature_intelligence8</th>\n",
              "      <th>feature_intelligence9</th>\n",
              "      <th>feature_intelligence10</th>\n",
              "      <th>feature_intelligence11</th>\n",
              "      <th>feature_intelligence12</th>\n",
              "      <th>feature_charisma1</th>\n",
              "      <th>feature_charisma2</th>\n",
              "      <th>feature_charisma3</th>\n",
              "      <th>feature_charisma4</th>\n",
              "      <th>feature_charisma5</th>\n",
              "      <th>feature_charisma6</th>\n",
              "      <th>feature_charisma7</th>\n",
              "      <th>feature_charisma8</th>\n",
              "      <th>feature_charisma9</th>\n",
              "      <th>feature_charisma10</th>\n",
              "      <th>feature_charisma11</th>\n",
              "      <th>feature_charisma12</th>\n",
              "      <th>feature_charisma13</th>\n",
              "      <th>feature_charisma14</th>\n",
              "      <th>feature_charisma15</th>\n",
              "      <th>feature_charisma16</th>\n",
              "      <th>feature_charisma17</th>\n",
              "      <th>feature_charisma18</th>\n",
              "      <th>feature_charisma19</th>\n",
              "      <th>feature_charisma20</th>\n",
              "      <th>feature_charisma21</th>\n",
              "      <th>feature_charisma22</th>\n",
              "      <th>feature_charisma23</th>\n",
              "      <th>feature_charisma24</th>\n",
              "      <th>feature_charisma25</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_wisdom8</th>\n",
              "      <th>feature_wisdom9</th>\n",
              "      <th>feature_wisdom10</th>\n",
              "      <th>feature_wisdom11</th>\n",
              "      <th>feature_wisdom12</th>\n",
              "      <th>feature_wisdom13</th>\n",
              "      <th>feature_wisdom14</th>\n",
              "      <th>feature_wisdom15</th>\n",
              "      <th>feature_wisdom16</th>\n",
              "      <th>feature_wisdom17</th>\n",
              "      <th>feature_wisdom18</th>\n",
              "      <th>feature_wisdom19</th>\n",
              "      <th>feature_wisdom20</th>\n",
              "      <th>feature_wisdom21</th>\n",
              "      <th>feature_wisdom22</th>\n",
              "      <th>feature_wisdom23</th>\n",
              "      <th>feature_wisdom24</th>\n",
              "      <th>feature_wisdom25</th>\n",
              "      <th>feature_wisdom26</th>\n",
              "      <th>feature_wisdom27</th>\n",
              "      <th>feature_wisdom28</th>\n",
              "      <th>feature_wisdom29</th>\n",
              "      <th>feature_wisdom30</th>\n",
              "      <th>feature_wisdom31</th>\n",
              "      <th>feature_wisdom32</th>\n",
              "      <th>feature_wisdom33</th>\n",
              "      <th>feature_wisdom34</th>\n",
              "      <th>feature_wisdom35</th>\n",
              "      <th>feature_wisdom36</th>\n",
              "      <th>feature_wisdom37</th>\n",
              "      <th>feature_wisdom38</th>\n",
              "      <th>feature_wisdom39</th>\n",
              "      <th>feature_wisdom40</th>\n",
              "      <th>feature_wisdom41</th>\n",
              "      <th>feature_wisdom42</th>\n",
              "      <th>feature_wisdom43</th>\n",
              "      <th>feature_wisdom44</th>\n",
              "      <th>feature_wisdom45</th>\n",
              "      <th>feature_wisdom46</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>n000315175b67977</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>n0014af834a96cdd</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>n001c93979ac41d4</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>n0034e4143f22a13</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>n00679d1a636062f</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 314 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id   era data_type  ...  feature_wisdom45  feature_wisdom46  target\n",
              "0  n000315175b67977  era1     train  ...              0.50              0.75    0.50\n",
              "1  n0014af834a96cdd  era1     train  ...              0.25              1.00    0.25\n",
              "2  n001c93979ac41d4  era1     train  ...              0.25              0.75    0.25\n",
              "3  n0034e4143f22a13  era1     train  ...              1.00              1.00    0.25\n",
              "4  n00679d1a636062f  era1     train  ...              0.25              0.75    0.75\n",
              "\n",
              "[5 rows x 314 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "Skw9H8Sd4xJs",
        "outputId": "0f0fb223-80cd-4e12-e54c-a1e644c87723"
      },
      "source": [
        "tourney_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>era</th>\n",
              "      <th>data_type</th>\n",
              "      <th>feature_intelligence1</th>\n",
              "      <th>feature_intelligence2</th>\n",
              "      <th>feature_intelligence3</th>\n",
              "      <th>feature_intelligence4</th>\n",
              "      <th>feature_intelligence5</th>\n",
              "      <th>feature_intelligence6</th>\n",
              "      <th>feature_intelligence7</th>\n",
              "      <th>feature_intelligence8</th>\n",
              "      <th>feature_intelligence9</th>\n",
              "      <th>feature_intelligence10</th>\n",
              "      <th>feature_intelligence11</th>\n",
              "      <th>feature_intelligence12</th>\n",
              "      <th>feature_charisma1</th>\n",
              "      <th>feature_charisma2</th>\n",
              "      <th>feature_charisma3</th>\n",
              "      <th>feature_charisma4</th>\n",
              "      <th>feature_charisma5</th>\n",
              "      <th>feature_charisma6</th>\n",
              "      <th>feature_charisma7</th>\n",
              "      <th>feature_charisma8</th>\n",
              "      <th>feature_charisma9</th>\n",
              "      <th>feature_charisma10</th>\n",
              "      <th>feature_charisma11</th>\n",
              "      <th>feature_charisma12</th>\n",
              "      <th>feature_charisma13</th>\n",
              "      <th>feature_charisma14</th>\n",
              "      <th>feature_charisma15</th>\n",
              "      <th>feature_charisma16</th>\n",
              "      <th>feature_charisma17</th>\n",
              "      <th>feature_charisma18</th>\n",
              "      <th>feature_charisma19</th>\n",
              "      <th>feature_charisma20</th>\n",
              "      <th>feature_charisma21</th>\n",
              "      <th>feature_charisma22</th>\n",
              "      <th>feature_charisma23</th>\n",
              "      <th>feature_charisma24</th>\n",
              "      <th>feature_charisma25</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_wisdom8</th>\n",
              "      <th>feature_wisdom9</th>\n",
              "      <th>feature_wisdom10</th>\n",
              "      <th>feature_wisdom11</th>\n",
              "      <th>feature_wisdom12</th>\n",
              "      <th>feature_wisdom13</th>\n",
              "      <th>feature_wisdom14</th>\n",
              "      <th>feature_wisdom15</th>\n",
              "      <th>feature_wisdom16</th>\n",
              "      <th>feature_wisdom17</th>\n",
              "      <th>feature_wisdom18</th>\n",
              "      <th>feature_wisdom19</th>\n",
              "      <th>feature_wisdom20</th>\n",
              "      <th>feature_wisdom21</th>\n",
              "      <th>feature_wisdom22</th>\n",
              "      <th>feature_wisdom23</th>\n",
              "      <th>feature_wisdom24</th>\n",
              "      <th>feature_wisdom25</th>\n",
              "      <th>feature_wisdom26</th>\n",
              "      <th>feature_wisdom27</th>\n",
              "      <th>feature_wisdom28</th>\n",
              "      <th>feature_wisdom29</th>\n",
              "      <th>feature_wisdom30</th>\n",
              "      <th>feature_wisdom31</th>\n",
              "      <th>feature_wisdom32</th>\n",
              "      <th>feature_wisdom33</th>\n",
              "      <th>feature_wisdom34</th>\n",
              "      <th>feature_wisdom35</th>\n",
              "      <th>feature_wisdom36</th>\n",
              "      <th>feature_wisdom37</th>\n",
              "      <th>feature_wisdom38</th>\n",
              "      <th>feature_wisdom39</th>\n",
              "      <th>feature_wisdom40</th>\n",
              "      <th>feature_wisdom41</th>\n",
              "      <th>feature_wisdom42</th>\n",
              "      <th>feature_wisdom43</th>\n",
              "      <th>feature_wisdom44</th>\n",
              "      <th>feature_wisdom45</th>\n",
              "      <th>feature_wisdom46</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>n0003aa52cab36c2</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>n000920ed083903f</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>n0038e640522c4a6</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>n004ac94a87dc54b</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>n0052fe97ea0c05f</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 314 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id     era  ... feature_wisdom46  target\n",
              "0  n0003aa52cab36c2  era121  ...             0.00    0.25\n",
              "1  n000920ed083903f  era121  ...             0.50    0.50\n",
              "2  n0038e640522c4a6  era121  ...             0.00    1.00\n",
              "3  n004ac94a87dc54b  era121  ...             0.25    0.50\n",
              "4  n0052fe97ea0c05f  era121  ...             1.00    0.75\n",
              "\n",
              "[5 rows x 314 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y8F2NgN4400",
        "outputId": "e411d329-20d6-4286-f4b6-a520b0a044c9"
      },
      "source": [
        "# Number of observations in each dataset type\n",
        "tourney_df['data_type'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "test          1555267\n",
              "validation     137779\n",
              "live             5435\n",
              "Name: data_type, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu9JcWlX5cnK"
      },
      "source": [
        "### Data Preprocessing\n",
        "Most of the data cleaning has been done by Numerai in order to anonymize and obfuscate the data to us. This is done purposefully because of the data sharing rights from the data vendors Numerai spends millions of dollars on (thank you Numerai!). \n",
        "\n",
        "Steps:\n",
        "* Extract features\n",
        "* Convert era from string to integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-0pxeCF5Cjd",
        "outputId": "45d81270-f34b-4e60-a5ae-3ba9d286afab"
      },
      "source": [
        "# Extract features\n",
        "tourney_ids = tourney_df['id']\n",
        "features = [c for c in tourney_df if c.startswith('feature')]\n",
        "\n",
        "# The training data is also grouped into 120 different eras (1-120)\n",
        "train_df[\"erano\"] = train_df[\"era\"].str.slice(3).astype(int)\n",
        "\n",
        "valid_df = tourney_df[tourney_df['data_type']=='validation']\n",
        "valid_df[\"erano\"] = valid_df[\"era\"].str.slice(3).astype(int)\n",
        "\n",
        "# Extract eras\n",
        "eras = train_df[\"erano\"]\n",
        "target = \"target\"\n",
        "\n",
        "print(\"Training:\", train_df.shape)\n",
        "print(\"Validation:\", valid_df.shape)\n",
        "print(\"Tournament:\", tourney_df.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: (501808, 315)\n",
            "Validation: (137779, 315)\n",
            "Tournament: (1698481, 314)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JE96P57dvwQ",
        "outputId": "1d97d0e2-5b26-4f10-8d58-782d3675d1a9"
      },
      "source": [
        "print(\"First five features:\")\n",
        "print(features[:5])\n",
        "print()\n",
        "print(\"Number of unique eras in training data: \", len(set(eras)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First five features:\n",
            "['feature_intelligence1', 'feature_intelligence2', 'feature_intelligence3', 'feature_intelligence4', 'feature_intelligence5']\n",
            "\n",
            "Number of unique eras in training data:  120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw3cmBTK6BHY"
      },
      "source": [
        "### Split the training data into training/testing sets\n",
        "Things to think about:\n",
        "* Is it useful to use all of the features?\n",
        "* How should we think about the different eras in the data?\n",
        "* Do some features matter more in particular eras than in other eras?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGkWzEgR5pY-",
        "outputId": "26e30d33-9225-44f3-d5de-f7f043e8a65f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_df[features], train_df[target],\n",
        "                                                    stratify=train_df['erano'],\n",
        "                                                    test_size=0.25, random_state=0)\n",
        "\n",
        "print(\"X_train size: \", X_train.shape)\n",
        "print(\"X_test_size: \", X_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train size:  (376356, 310)\n",
            "X_test_size:  (125452, 310)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD0lIZabc4tM"
      },
      "source": [
        "Here, I decided to stratify the training dataset based on era ( \"stratify=train_df['erano']\" ). This allows the model to be trained on observations from all eras.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_UkZFcI6jLR"
      },
      "source": [
        "### Dimensionality Reduction Techniques\n",
        "\n",
        "* PCA\n",
        "* K-Means Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uViYWuplkPp4"
      },
      "source": [
        "PCA\n",
        "* You can change the number of components or % variance retained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phzAI9HofgVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9918b61-686d-4fa0-ec2b-715a538e93e0"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# You can specify the number of components\n",
        "pca = PCA(n_components=120)\n",
        "pca_train = pca.fit_transform(X_train)\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(np.cumsum(explained_variance))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.10418853 0.18438231 0.23781067 0.27816663 0.31218791 0.34198411\n",
            " 0.36653258 0.38880232 0.40842065 0.42659024 0.44218313 0.45664365\n",
            " 0.47012225 0.48294476 0.49506729 0.50705258 0.51851441 0.52949213\n",
            " 0.54032917 0.55054991 0.56046169 0.57020912 0.57954533 0.58874189\n",
            " 0.59757115 0.60587842 0.61391695 0.62189413 0.62958392 0.63701823\n",
            " 0.64435391 0.65151837 0.65841186 0.66521867 0.67182625 0.67840565\n",
            " 0.68468038 0.69087394 0.69692735 0.70279109 0.70852448 0.71415297\n",
            " 0.71968002 0.72509363 0.73040029 0.73557974 0.74063086 0.7455087\n",
            " 0.75029001 0.75503039 0.75968773 0.76425439 0.76874844 0.77320189\n",
            " 0.77752845 0.78179287 0.78598474 0.79013674 0.7941559  0.79816099\n",
            " 0.80210175 0.80595474 0.80971851 0.81343184 0.81707693 0.82065522\n",
            " 0.82411792 0.82754486 0.83093956 0.83425678 0.83753007 0.84079187\n",
            " 0.84399372 0.84716222 0.85028017 0.85329624 0.85628085 0.85919775\n",
            " 0.86204941 0.8648169  0.86754742 0.87024008 0.87286847 0.87547826\n",
            " 0.87806157 0.88059975 0.88309048 0.885535   0.88794691 0.8902847\n",
            " 0.89259527 0.894758   0.89689087 0.89896214 0.90099904 0.90300098\n",
            " 0.90496643 0.90690717 0.90881233 0.91067222 0.91243985 0.91419602\n",
            " 0.91594239 0.91765029 0.91930041 0.92092904 0.92250197 0.92405516\n",
            " 0.92557448 0.92705187 0.92850455 0.92991551 0.9312982  0.93267702\n",
            " 0.93404886 0.93539463 0.93670259 0.93797617 0.93924552 0.94045887]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MstCOZRqfgKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3132d991-0e03-4c5a-e84e-040cf87ec7b5"
      },
      "source": [
        "# You could also specify the % of variance you want to retain\n",
        "pca = PCA(n_components=0.90)\n",
        "pca_train = pca.fit_transform(X_train)\n",
        "\n",
        "# Explained variance of each component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"Number of components with % variance: \", len(explained_variance))\n",
        "print(np.cumsum(explained_variance))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of components with % variance:  95\n",
            "[0.10418853 0.18438231 0.23781067 0.27816663 0.31218791 0.34198411\n",
            " 0.36653258 0.38880232 0.40842065 0.42659024 0.44218313 0.45664365\n",
            " 0.47012225 0.48294476 0.49506729 0.50705258 0.51851441 0.52949213\n",
            " 0.54032917 0.55054991 0.56046169 0.57020912 0.57954533 0.58874189\n",
            " 0.59757115 0.60587842 0.61391695 0.62189414 0.62958392 0.63701823\n",
            " 0.64435391 0.65151837 0.65841186 0.66521867 0.67182625 0.67840565\n",
            " 0.68468038 0.69087394 0.69692735 0.7027911  0.70852448 0.71415298\n",
            " 0.71968003 0.72509363 0.7304003  0.73557974 0.74063087 0.7455087\n",
            " 0.75029003 0.75503041 0.75968774 0.76425441 0.76874846 0.77320192\n",
            " 0.77752848 0.78179291 0.78598479 0.79013678 0.79415595 0.79816105\n",
            " 0.80210185 0.80595484 0.80971862 0.81343196 0.81707707 0.82065537\n",
            " 0.82411808 0.82754506 0.83093977 0.83425703 0.83753036 0.84079221\n",
            " 0.84399408 0.84716267 0.85028064 0.85329674 0.85628139 0.85919834\n",
            " 0.86205015 0.86481775 0.86754834 0.87024136 0.87286978 0.87547995\n",
            " 0.87806388 0.88060228 0.88309324 0.88553807 0.88795012 0.89028855\n",
            " 0.89259925 0.8947623  0.89689735 0.89896888 0.9010064 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHEMSeWOnBd0",
        "outputId": "76659127-cfc3-498d-9cc8-7db0cf50a5d5"
      },
      "source": [
        "print(\"Original X_train shape:\", X_train.shape)\n",
        "print(\"PCA Transformed X_train shape:\", pca_train.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original X_train shape: (376356, 310)\n",
            "PCA Transformed X_train shape: (376356, 95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ickvZGnskL8O"
      },
      "source": [
        "K-Means Clustering\n",
        "* You can change the number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOQbsTxgkRMT"
      },
      "source": [
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "\n",
        "kmeans120 = MiniBatchKMeans(n_clusters=120, random_state=6).fit(X_train)\n",
        "kmeans120_train = kmeans120.transform(X_train)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJQyAY7km4AH",
        "outputId": "07a9ff05-b06f-44dc-8162-51f848be9832"
      },
      "source": [
        "print(\"Original X_train shape:\", X_train.shape)\n",
        "print(\"K-Means Clustered X_train shape:\", kmeans120_train.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original X_train shape: (376356, 310)\n",
            "K-Means Clustered X_train shape: (376356, 120)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95RQV4SC8WQB"
      },
      "source": [
        "### Bayesian Optimization for xgboost\n",
        "* I am going to use the PCA transformed training data but you can easily switch it to PCA or K-means clustering\n",
        "* set 'tree_method':'gpu_hist' to train on GPU for faster training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caV5K2XLbIKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd166163-e431-44ed-b099-2a0f23779dc6"
      },
      "source": [
        "# Download bayesian-optimization library\n",
        "! pip install bayesian-optimization"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7a/fd8059a3881d3ab37ac8f72f56b73937a14e8bb14a9733e68cc8b17dbe3c/bayesian-optimization-1.2.0.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.0.1)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-cp37-none-any.whl size=11687 sha256=f6706ee1a344ba882d8b409d370335fee42c204c34f09f17e252a90d960e6b15\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/56/ae/e0e3c1fc1954dc3ec712e2df547235ed072b448094d8f94aec\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGr3FR0f8In_"
      },
      "source": [
        "import xgboost\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# Convert autoencoded training data to DMatrix for XGBoost\n",
        "dtrain = xgboost.DMatrix(pca_train, y_train)\n",
        " \n",
        "def bo_tune_xgb(max_depth, gamma, learning_rate, subsample, colsample_bytree, min_child_weight, n_estimators, alpha, eta):\n",
        "    params = {'max_depth': int(max_depth),\n",
        "              'gamma': gamma,\n",
        "              'learning_rate': learning_rate,\n",
        "              'subsample': subsample,\n",
        "              'colsample_bytree': colsample_bytree,\n",
        "              'min_child_weight': min_child_weight,\n",
        "              'n_estimators': int(n_estimators),\n",
        "              'alpha': alpha,\n",
        "              'eta': eta,\n",
        "              'eval_metric': 'rmse',\n",
        "              'tree_method': 'gpu_hist'}\n",
        " \n",
        "    #Cross validating with the specified parameters in 3 folds\n",
        "    cv_result = xgboost.cv(params, dtrain, nfold=3)\n",
        "    #Return the negative RMSE\n",
        "    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcLmno4v8sr-"
      },
      "source": [
        "#### n_iter \n",
        "* controls how many bayesian optimization steps to perform, more steps = more likely to find a good maximization\n",
        "\n",
        "#### init_points \n",
        "* controls how many steps of random exploration you want to perform to help diversify the exploration space\n",
        "\n",
        "#### acquisition functions\n",
        "* decides how to guide the optimization\n",
        "* acq: {'ucb', 'ei', 'poi'}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMLIAiJp8mIY",
        "outputId": "5171fbcc-68c6-4ad3-a44f-c340303c0d89"
      },
      "source": [
        "# Initial Bayesian Optimization Search\n",
        "xgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (4,12),\n",
        "                                             'gamma': (0.1, 1),\n",
        "                                             'subsample' : (0.5, 1),\n",
        "                                             'learning_rate' : (0.0001, 0.01),\n",
        "                                             'colsample_bytree': (0.7, 1),\n",
        "                                             'min_child_weight': (4, 10),\n",
        "                                             'n_estimators':(80, 240),\n",
        "                                             'alpha': (0.1, 1),\n",
        "                                             'eta': (0.1, 0.3)\n",
        "                                            })\n",
        "\n",
        "xgb_bo.maximize(n_iter=5, init_points=5, acq='ei')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   |   alpha   | colsam... |    eta    |   gamma   | learni... | max_depth | min_ch... | n_esti... | subsample |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.2233  \u001b[0m | \u001b[0m 0.545   \u001b[0m | \u001b[0m 0.8208  \u001b[0m | \u001b[0m 0.2227  \u001b[0m | \u001b[0m 0.4392  \u001b[0m | \u001b[0m 0.000462\u001b[0m | \u001b[0m 11.23   \u001b[0m | \u001b[0m 5.489   \u001b[0m | \u001b[0m 126.2   \u001b[0m | \u001b[0m 0.8049  \u001b[0m |\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.2233  \u001b[0m | \u001b[95m 0.3385  \u001b[0m | \u001b[95m 0.7712  \u001b[0m | \u001b[95m 0.1272  \u001b[0m | \u001b[95m 0.9809  \u001b[0m | \u001b[95m 0.002895\u001b[0m | \u001b[95m 9.267   \u001b[0m | \u001b[95m 4.973   \u001b[0m | \u001b[95m 94.29   \u001b[0m | \u001b[95m 0.8378  \u001b[0m |\n",
            "| \u001b[95m 3       \u001b[0m | \u001b[95m-0.2233  \u001b[0m | \u001b[95m 0.8811  \u001b[0m | \u001b[95m 0.8899  \u001b[0m | \u001b[95m 0.1431  \u001b[0m | \u001b[95m 0.8343  \u001b[0m | \u001b[95m 0.004421\u001b[0m | \u001b[95m 9.021   \u001b[0m | \u001b[95m 4.626   \u001b[0m | \u001b[95m 107.4   \u001b[0m | \u001b[95m 0.8969  \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.2233  \u001b[0m | \u001b[0m 0.3628  \u001b[0m | \u001b[0m 0.7349  \u001b[0m | \u001b[0m 0.1687  \u001b[0m | \u001b[0m 0.2585  \u001b[0m | \u001b[0m 0.000533\u001b[0m | \u001b[0m 6.394   \u001b[0m | \u001b[0m 6.673   \u001b[0m | \u001b[0m 218.7   \u001b[0m | \u001b[0m 0.9642  \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.2233  \u001b[0m | \u001b[0m 0.1104  \u001b[0m | \u001b[0m 0.7302  \u001b[0m | \u001b[0m 0.1434  \u001b[0m | \u001b[0m 0.4141  \u001b[0m | \u001b[0m 0.000811\u001b[0m | \u001b[0m 10.83   \u001b[0m | \u001b[0m 7.839   \u001b[0m | \u001b[0m 205.7   \u001b[0m | \u001b[0m 0.8813  \u001b[0m |\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.2233  \u001b[0m | \u001b[0m 0.6334  \u001b[0m | \u001b[0m 0.9031  \u001b[0m | \u001b[0m 0.2209  \u001b[0m | \u001b[0m 0.7335  \u001b[0m | \u001b[0m 0.003866\u001b[0m | \u001b[0m 4.276   \u001b[0m | \u001b[0m 6.817   \u001b[0m | \u001b[0m 239.9   \u001b[0m | \u001b[0m 0.9562  \u001b[0m |\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.2233  \u001b[0m | \u001b[0m 0.4063  \u001b[0m | \u001b[0m 0.7977  \u001b[0m | \u001b[0m 0.1852  \u001b[0m | \u001b[0m 0.2353  \u001b[0m | \u001b[0m 0.003267\u001b[0m | \u001b[0m 4.811   \u001b[0m | \u001b[0m 7.244   \u001b[0m | \u001b[0m 80.24   \u001b[0m | \u001b[0m 0.6557  \u001b[0m |\n",
            "| \u001b[95m 8       \u001b[0m | \u001b[95m-0.2233  \u001b[0m | \u001b[95m 0.9475  \u001b[0m | \u001b[95m 0.7434  \u001b[0m | \u001b[95m 0.2909  \u001b[0m | \u001b[95m 0.1703  \u001b[0m | \u001b[95m 0.006034\u001b[0m | \u001b[95m 11.53   \u001b[0m | \u001b[95m 4.109   \u001b[0m | \u001b[95m 239.8   \u001b[0m | \u001b[95m 0.7703  \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.2233  \u001b[0m | \u001b[0m 0.4771  \u001b[0m | \u001b[0m 0.9031  \u001b[0m | \u001b[0m 0.2822  \u001b[0m | \u001b[0m 0.3936  \u001b[0m | \u001b[0m 0.003163\u001b[0m | \u001b[0m 11.5    \u001b[0m | \u001b[0m 5.049   \u001b[0m | \u001b[0m 232.5   \u001b[0m | \u001b[0m 0.6288  \u001b[0m |\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.2233  \u001b[0m | \u001b[0m 0.5185  \u001b[0m | \u001b[0m 0.8546  \u001b[0m | \u001b[0m 0.1041  \u001b[0m | \u001b[0m 0.247   \u001b[0m | \u001b[0m 0.002533\u001b[0m | \u001b[0m 5.999   \u001b[0m | \u001b[0m 9.926   \u001b[0m | \u001b[0m 80.09   \u001b[0m | \u001b[0m 0.8252  \u001b[0m |\n",
            "=====================================================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPAb6BLmuRGk"
      },
      "source": [
        "Just for this notebook I kept the number of iterations and initiation points relatively small so it doesn't a long time to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK-8zuhP8_QN",
        "outputId": "95abcc3c-89d7-407c-cbf4-b5348f859b11"
      },
      "source": [
        "# Obtain best parameters from bayesian optimization search\n",
        "\n",
        "params = xgb_bo.max['params']\n",
        "#Converting the max_depth and n_estimator values from float to int\n",
        "params['max_depth']= int(round(params['max_depth']))\n",
        "params['n_estimators']= int(round(params['n_estimators']))\n",
        "params"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alpha': 0.9475032339413909,\n",
              " 'colsample_bytree': 0.7433824908679298,\n",
              " 'eta': 0.29089610664583754,\n",
              " 'gamma': 0.1703160779480699,\n",
              " 'learning_rate': 0.006033561968914174,\n",
              " 'max_depth': 12,\n",
              " 'min_child_weight': 4.10853248047008,\n",
              " 'n_estimators': 240,\n",
              " 'subsample': 0.7703410033330202}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eXTChi_Cve4",
        "outputId": "fd300d58-9079-41c3-e35e-a9a588461912"
      },
      "source": [
        "#Train XGBRegressor with best params from bayesian optimization\n",
        "pca_X_train = pca.transform(X_train)\n",
        "xgb_bestparams = xgboost.XGBRegressor(**params, tree_method='gpu_hist').fit(pca_X_train, y_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[18:54:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPxZGXWTq2MJ"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_QBkGoOq5O8"
      },
      "source": [
        "# The models should be scored based on the rank-correlation (spearman) with the target\n",
        "def numerai_score(y_true, y_pred):\n",
        "    rank_pred = y_pred.groupby(eras).apply(lambda x: x.rank(pct=True, method=\"first\"))\n",
        "    return np.corrcoef(y_true, rank_pred)[0,1]\n",
        "\n",
        "# It can also be convenient while working to evaluate based on the regular (pearson) correlation\n",
        "def correlation_score(y_true, y_pred):\n",
        "    return np.corrcoef(y_true, y_pred)[0,1]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN1KiBPqFQp2",
        "outputId": "2c270e0e-6f6b-4366-a5b7-e453d29a593c"
      },
      "source": [
        "train_preds = xgb_bestparams.predict(pca_X_train)\n",
        "print('Training Scores')\n",
        "print('Numerai Score: ', numerai_score(y_train, pd.Series(train_preds)))\n",
        "print('Correlation Score: ', correlation_score(y_train, pd.Series(train_preds)))\n",
        "print()\n",
        "\n",
        "pca_X_test = pca.transform(X_test)\n",
        "test_preds = xgb_bestparams.predict(pca_X_test)\n",
        "print('Testing Scores')\n",
        "print('Numerai Score: ', numerai_score(y_test, pd.Series(test_preds)))\n",
        "print('Correlation Score: ', correlation_score(y_test, pd.Series(test_preds)))\n",
        "print()\n",
        "\n",
        "pca_validation = pca.transform(valid_df[features])\n",
        "validation_preds = xgb_bestparams.predict(pca_validation)\n",
        "print('Validation Scores')\n",
        "print('Numerai Score: ', numerai_score(valid_df[target], pd.Series(validation_preds)))\n",
        "print('Correlation Score: ', correlation_score(valid_df[target], pd.Series(validation_preds)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Scores\n",
            "Numerai Score:  0.7876765510655394\n",
            "Correlation Score:  0.7946813926423871\n",
            "\n",
            "Testing Scores\n",
            "Numerai Score:  0.024828261534256667\n",
            "Correlation Score:  0.024012714552232948\n",
            "\n",
            "Validation Scores\n",
            "Numerai Score:  0.008600048036441911\n",
            "Correlation Score:  0.009102965481810055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RXO8SeqpcdW"
      },
      "source": [
        "The model clearly overfits the training data because the numerai and correlation scores of the testing and validation sets drop off significantly from the training set scores. This means that the model will most likely not generalize well to the live tournament data. This model might perform well in a couple of rounds, but over time I predict it will generally underperform. \n",
        "\n",
        "Personally, the best models I have made that are performing well on the live tournament typically have validation scores between 0.03-0.05 and slightly overfit the training data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD0bYsH5q1O5"
      },
      "source": [
        "### Predicting the Live Tournament Data\n",
        "* Recall we read in the tournament data at the beginning of the notebook under tourney_df\n",
        "* Make sure the format is correct (id, prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHyp159OFWF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8761fed0-36bb-47ba-ef11-7739b48b2664"
      },
      "source": [
        "# Apply autoencoder for dimensionality reduction to the tournament data\n",
        "pca_tourney = pca.transform(tourney_df[features])\n",
        "\n",
        "# Use our best xgboost model from bayesian optimization to predict autoencoded tournament data\n",
        "# Avoid RAM issues by splitting tourney data\n",
        "tourney_preds_1 = xgb_bestparams.predict(pca_tourney[:1000000])\n",
        "tourney_preds_2 = xgb_bestparams.predict(pca_tourney[1000000:])\n",
        "tourney_preds = np.concatenate((tourney_preds_1, tourney_preds_2))\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['id'] = tourney_ids\n",
        "df['prediction'] = tourney_preds\n",
        "print(\"Current round: \", current_round)\n",
        "df.info()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current round:  260\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1698481 entries, 0 to 1698480\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Dtype  \n",
            "---  ------      -----  \n",
            " 0   id          object \n",
            " 1   prediction  float32\n",
            "dtypes: float32(1), object(1)\n",
            "memory usage: 19.4+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "q6RGlHpHr9y6",
        "outputId": "8cc40eeb-421c-42aa-fb8d-79f7078b2004"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>n0003aa52cab36c2</td>\n",
              "      <td>0.494618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>n000920ed083903f</td>\n",
              "      <td>0.500563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>n0038e640522c4a6</td>\n",
              "      <td>0.511354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>n004ac94a87dc54b</td>\n",
              "      <td>0.503960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>n0052fe97ea0c05f</td>\n",
              "      <td>0.498667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  prediction\n",
              "0  n0003aa52cab36c2    0.494618\n",
              "1  n000920ed083903f    0.500563\n",
              "2  n0038e640522c4a6    0.511354\n",
              "3  n004ac94a87dc54b    0.503960\n",
              "4  n0052fe97ea0c05f    0.498667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "6TZm2nn7DeUn",
        "outputId": "0d11b7fb-d0ea-497e-95f7-e54d6ab79738"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.displot(df['prediction'])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-23 18:55:24,083 INFO numexpr.utils: NumExpr defaulting to 4 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.FacetGrid at 0x7fd333980bd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SiWGuvw5xWx"
      },
      "source": [
        "# Save as csv file you can upload on the numerai website\n",
        "df.to_csv(f'round{current_round}_pca_xgb_bayesopt_predictions.csv', index=False)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KgdgFVGrtC_"
      },
      "source": [
        "### Documentation and References\n",
        "\n",
        "Numerapi: https://github.com/numerai/numerapi\n",
        "\n",
        "Numerai Examples: https://github.com/numerai/example-scripts\n",
        "\n",
        "Bayesian Optimization: https://github.com/fmfn/BayesianOptimization\n",
        "\n",
        "XGBoost: https://github.com/dmlc/xgboost/tree/master/demo/guide-python"
      ]
    }
  ]
}